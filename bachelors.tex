%%
%%  AaltoTheses - LaTeX-tutkielmapohjat Aalto-tyylille
%%
%%  Hau Phan
%%  hau.phan@aalto.fi
%%
\begin{filecontents*}{\jobname.xmpdata}
  \Title{Normalizing Flows for Graph Generation}
  \Author{Hannu Tiitu}
  \Keywords{graph neural network\sep normalizing flows\sep continuous normalizing flows\sep graphs generation\sep machine learning\sep geometric machine learning}
  \Publisher{Aalto University}
\end{filecontents*}
\documentclass[oneside,pdfa]{aaltoseries}
\makeatletter
\@ifpackageloaded{inputenc}{%
  \inputencoding{utf8}}{%
  \usepackage[utf8]{inputenc}}
\hypersetup{hidelinks}                % Linkkien korostus pois
\makeatother
\usepackage[finnish,english]{babel}   % Kieli on englanti, tiivistelmässä suomi
\usepackage{setspace}                 % Rivivälin säätämiseksi
\usepackage{afterpage}                % Sivun taustaväri
\microtypesetup{letterspace=25}       % Kannen harvaan välistykseen
\usepackage{lastpage}



\author{Hau Phan}
\title{Normalizing Flows for Graph Generation}

\begin{document}



%%  KANSI  ---------------------------------------------

\thispagestyle{empty}
\setcounter{page}{0}  % Kansisivulle sivunumero 0

% Kansisivun marginaalit
\newgeometry{left=23.2mm,right=23.2mm,top=13.5mm,bottom=18mm}

% Punainen kansisivu
\pagecolor{aaltoRed}\afterpage{\nopagecolor}
{\color{black}  % Musta teksti

	{\parindent0pt % Kappaleiden sisennys pois päältä
		{\fontsize{11.9pt}{11.9pt}\bfseries\sffamily\lsstyle Bachelor’s Programme in Science and Technology}

		\color{white}  % Valkoinen teksti alkaa

		\vspace{13.1mm}

		\begin{spacing}{3.1}
			{\fontsize{35}{35}\selectfont Normalizing Flows \\ for Graph Generation}
		\end{spacing}

		\vspace{2.2mm}

		\begin{spacing}{1.24}
			{\fontsize{14pt}{14pt}\bfseries\sffamily\lsstyle Literature review}
		\end{spacing}

		\vspace{7.2mm}

		\rule{\textwidth}{1.25pt}

		\vspace{8.5mm}

		{\fontsize{13.9pt}{13.9pt}\bfseries\sffamily\lsstyle Hau Phan}

		\vfill

		\begin{picture}(0,0)
			\put(356,-7.8){\bfseries\sffamily\footnotesize\lsstyle BACHELOR'S}
			\put(356,-17.4){\bfseries\sffamily\footnotesize\lsstyle THESIS}
			\put(346,-26.5){\rule{.75pt}{25pt}}
		\end{picture}

		\AaltoLogoSmall{.66}{?}{white}

	} % Kappaleiden sisennys takaisin käyttöön
} % Valkoisen tekstin pääätös



%%  NIMIÖSIVU  -----------------------------------------

\newpage

\pagenumbering{roman}

% Nimiösivun marginaalit
\newgeometry{left=80.7mm,right=25mm,top=12.9mm,bottom=21mm}

\thispagestyle{empty}

{\parindent0pt % Kappaleiden sisennys pois päältä
	\begin{spacing}{1.1}
		\hspace{-39.1mm}{\fontsize{10.5pt}{10.5pt}\sffamily\lsstyle Aalto University}

		\hspace{-39.1mm}{\fontsize{10.5pt}{10.5pt}\bfseries\sffamily\lsstyle BACHELOR'S THESIS} {\sffamily\lsstyle \the\year}
	\end{spacing}

	\vspace{12.7mm}

	\begin{spacing}{1.63}
		{\fontsize{17.8pt}{17.8pt}\selectfont Normalizing Flows \\for Graph Generation}
	\end{spacing}

	\vspace{10.5mm}

	\begin{spacing}{1.2}
		{\fontsize{13pt}{13pt}\selectfont Literature review}
	\end{spacing}

	\vspace{10.6mm}

	{\fontsize{13.9pt}{13.9pt}\bfseries\sffamily\lsstyle Hau Phan}

	\vfill

	{\fontsize{10.3pt}{10.3pt}\sffamily\lsstyle\raggedright
		\begin{spacing}{1.06}

			Thesis submitted in partial fulfillment of the requirements for the
			degree of Bachelor of Science in Technology.

			Otaniemi, \today

			\begin{tabbing}
				Supervisor:\hspace{6mm} \= Anirudh Jain \\
				Advisor: \> Anirudh Jain
			\end{tabbing}
			\vspace{-4mm}
		\end{spacing}
	} % fontsize

	\vspace{11.5mm}

	\begin{spacing}{.9}
		{\bfseries\sffamily\lsstyle Aalto University \\
			School of Science \\
			Bachelor’s Programme in Science and Technology}
	\end{spacing}
} % Kappaleiden sisennys takaisin käyttöön



%%  ABSTRACT  ------------------------------------------

\newpage
\phantomsection
\addcontentsline{toc}{chapter}{Abstract}

% Tiivistelmien marginaalit
\newgeometry{left=41.8mm,right=25mm,top=14.33mm,bottom=27mm}
% Alkuperäisessä Aalto-sarjassa marginaalit ovat suunnilleen näin:
%\newgeometry{left=41.8mm,right=17.6mm,top=14.33mm,bottom=20.4mm}

\begin{spacing}{.88}

	{\parindent0pt % Kappaleiden sisennys pois päältä
	\AaltoLogoSmall{.625}{''}{aaltoBlack}

	{\fontsize{13.9pt}{13.9pt}\selectfont
		\vspace{-8.9mm}\hfill{\bfseries\sffamily\lsstyle Abstract}}

	{\fontsize{9.48pt}{9.48pt}\selectfont
		\vspace{.9mm}\hfill{\bfseries\sffamily\lsstyle Aalto University, P.O. Box 11000, FI-00076 Aalto~~\textcolor{aaltoGray}{www.aalto.fi}}}

	\vspace{7.8mm}{\fontsize{10.5pt}{10.5pt}\bfseries\sffamily\lsstyle Author}\\
	{\small Hau Phan}

	\vspace{-2.4mm}\rule{\textwidth}{.75pt}

	{\fontsize{10.5pt}{10.5pt}\bfseries\sffamily\lsstyle Title}\\
	\parbox[t]{\textwidth}{\raggedright\small Normalizing Flows for Graph Generation}

	\vspace{.5mm}\rule{\textwidth}{.75pt}

	{\fontsize{10.5pt}{10.5pt}\bfseries\sffamily\lsstyle School}~~{\small School of Science}

	\vspace{-2.4mm}\rule{\textwidth}{.75pt}

	{\fontsize{10.5pt}{10.5pt}\bfseries\sffamily\lsstyle Degree programme}~~{\small Bachelor’s Programme in Science and Technology}

	\vspace{-2.4mm}\rule{\textwidth}{.75pt}

	{\fontsize{10.5pt}{10.5pt}\bfseries\sffamily\lsstyle Major}~~{\small Data Science }\hfill{\fontsize{10.5pt}{10.5pt}\bfseries\sffamily\lsstyle Code}~~{\small IL3011}

	\vspace{-2.4mm}\rule{\textwidth}{.75pt}

	{\fontsize{10.5pt}{10.5pt}\bfseries\sffamily\lsstyle Supervisor}~~{\small Anirudh Jain}

	\vspace{-2.4mm}\rule{\textwidth}{.75pt}

	{\fontsize{10.5pt}{10.5pt}\bfseries\sffamily\lsstyle Advisor}~~{\small Anirudh Jain}

	\vspace{-2.4mm}\rule{\textwidth}{.75pt}

	{\fontsize{10.5pt}{10.5pt}\bfseries\sffamily\lsstyle Level}~~{\small Bachelor's thesis}\hfill{\fontsize{10.5pt}{10.5pt}\bfseries\sffamily\lsstyle Date}~~{\small \today}\hfill{\fontsize{10.5pt}{10.5pt}\bfseries\sffamily\lsstyle Pages}~~{\small \pageref{LastPage}}\hfill{\fontsize{10.5pt}{10.5pt}\bfseries\sffamily\lsstyle Language}~~{\small English}

	\vspace{-2.4mm}\rule{\textwidth}{.75pt}

	\vspace{6mm}

	} % Kappaleiden sisennys takaisin käyttöön
\end{spacing}
\begin{spacing}{1.05}

	\noindent{\fontsize{10.5pt}{10.5pt}\bfseries\sffamily\lsstyle Abstract}
	\vspace{.8mm}

	{\small
		Lorem ipsum dolor sit amet, consectetuer adipiscing elit. Ut purus
		elit, vestibulum ut, placerat ac, adipiscing vitae, felis. Curabitur
		dictum gravida mauris. Nam arcu libero, nonummy eget, consectetuer
		id, vulputate a, magna. Donec vehicula augue eu neque. Pellentesque
		habitant morbi tristique senectus et netus et malesuada fames ac
		turpis egestas. Mauris ut leo. Cras viverra metus rhoncus sem. Nulla
		et lectus vestibulum urna fringilla ultrices. Phasellus eu tellus
		sit amet tortor gravida placerat. Integer sapien est, iaculis in,
		pretium quis, viverra ac, nunc. Praesent eget sem vel leo ultrices
		bibendum. Aenean faucibus. Morbi dolor nulla, malesuada eu, pulvinar
		at, mollis ac, nulla. Curabitur auctor semper nulla.  Donec varius
		orci eget risus. Duis nibh mi, congue eu, accumsan eleifend,
		sagittis quis, diam. Duis eget orci sit amet orci dignissim rutrum.

		Nam dui ligula, fringilla a, euismod sodales, sollicitudin vel,
		wisi. Morbi auctor lorem non justo. Nam lacus libero, pretium at,
		lobortis vitae, ultricies et, tellus. Donec aliquet, tortor sed
		accumsan bibendum, erat ligula aliquet magna, vitae ornare odio
		metus a mi. Morbi ac orci et nisl hendrerit mollis. Suspendisse ut
		massa. Cras nec ante. Pellentesque a nulla.  Cum sociis natoque
		penatibus et magnis dis parturient montes, nascetur ridiculus
		mus. Aliquam tincidunt urna. Nulla ullamcorper vestibulum
		turpis. Pellentesque cursus luctus mauris.

		Nulla malesuada porttitor diam. Donec felis erat, congue non,
		volutpat at, tincidunt tristique, libero. Vivamus viverra fermentum
		felis. Donec nonummy pellentesque ante. Phasellus adipiscing semper
		elit. Proin fermentum massa ac quam. Sed diam turpis, molestie
		vitae, placerat a, molestie nec, leo. Maecenas lacinia. Nam ipsum
		ligula, eleifend at, accumsan nec, suscipit a, ipsum. Morbi blandit
		ligula feugiat magna. Nunc eleifend consequat lorem. Sed lacinia
		nulla vitae enim. Pellentesque tincidunt purus vel magna. Integer
		non enim. Praesent euismod nunc eu purus.  Donec bibendum quam in
		tellus. Nullam cursus pulvinar lectus. Donec et mi. Nam vulputate
		metus eu enim. Vestibulum pellentesque felis eu massa.
	}

	\vfill

\end{spacing}
\begin{spacing}{.88}
	{\parindent0pt % Kappaleiden sisennys pois päältä

	\makebox[19mm][l]{\fontsize{10.5pt}{10.5pt}\bfseries\sffamily\lsstyle Keywords}\parbox[t]{123.6mm}{\raggedright\small graph neural network, normalizing flows, continuous normalizing flows, graphs generation, machine learning, geometric machine learning}

	\vspace{.5mm}\rule{\textwidth}{.75pt}

	{\fontsize{10.5pt}{10.5pt}\bfseries\sffamily\lsstyle urn}~~{\small https://aaltodoc.aalto.fi}

	\vspace{-2.4mm}\rule{\textwidth}{.75pt}

	} % Kappaleiden sisennys takaisin käyttöön
\end{spacing}



%%  SISÄLTÖ  -------------------------------------------

\newpage

\tableofcontents

%%  TYÖ ALKAA TÄSTÄ  -----------------------------------

\newpage

\pagenumbering{arabic}

\chapter*{Introduction}

\section{Motivation}

Deep learning has exploded in popularity in recent years due to its
effectiveness in capturing hidden patterns of Euclidean data such as images,
audio and natural languages. However, not all data can be represented in
Euclidean space \cite{bronsteinGeometricDeepLearning2017}.  There is an
increasing number of application for data that are represented as discrete
graphs: sets of nodes and edges that can directly model abstract relations
between objects. For example, molecules are essentially connected atoms, bounded
to one another by chemical bonds; users-products graphs are bipartite graphs
that imply preferences of a customer towards specific sets of products; citation
networks represents citationships between academic papers and inter/cross
connections between domains; road networks and traffics flows can also be
considered as graphs with additional attributes
\cite{wuComprehensiveSurveyGraph2021}. Graphs are common since they model
relationships, which are abstract and ubiquitous in nature. However, graphs are
also complex mathematical object: they can be irregular, have arbitrary numbers
of nodes and vertices, each node may have any number of neighbors; they are also
invariant with respect to rotation and translation, since there are no markers
of direction or origin with graphs \cite{bronsteinGeometricDeepLearning2017}.
This is in direct contrast to Euclidean data such as images, where the notion of
directions and rotations are straightforward to define. The complexity that
comes along the expressive power of graphs has imposed significant challenges to
existing machine learning methods such as convolution, which are easy to compute
in the image domain but difficult in graph domain
\cite{wuComprehensiveSurveyGraph2021}. Graph Neural Networks (GNNs)
\cite{scarselliGraphNeuralNetwork2009} are the first architecture to directly
operate on graphs, bridging the gap between discrete space and Euclidean space,
yet permeate the flexibility of composite neural network e.g. stacking and
residual connections. GNN and its derivatives had achieved significant success
in various supervised and unsupervised learning tasks with graphs
\cite{kipfSemiSupervisedClassificationGraph2017,
	velickovicGraphAttentionNetworks2018}.


An interesting subdomain of graph machine learning are generative model for
graphs. Generative modeling in particular has been popularized to various
modalities since the introduction of Generative Adversarial Network (GANs)
\cite{goodfellowGenerativeAdversarialNetworks2014} and Variational Autoencoder
(VAEs) \cite{kingmaAutoEncodingVariationalBayes2014}, where both had shown
remarkable success in capturing the data distribution of images and even audio
\cite{hersheyCNNArchitecturesLargescale2017}. For graphs generation in
particular, the majority of research efforts are focused on solving the
molecular graph generation problem, which have high practical value in drug
discovery \cite{wuComprehensiveSurveyGraph2021}. Specifically, there are two
primary way for generating new molecular graphs that are currently explored:
sequentially and globally. Sequential approaches generate graphs by proposing
nodes and edges step by step, while global approaches output a graph all at
once. Pioneers in sequential generation of graphs include graph generation using
SMILE representation of graphs
\cite{daiSyntaxdirectedVariationalAutoencoder2018,
	kusnerGrammarVariationalAutoencoder2017,
	gomez-bombarelliAutomaticChemicalDesign2018}, DeepGMG
\colucite{liLearningDeepGenerative2018} and GraphRNN
\cite{youGraphRNNGeneratingRealistic2018}. For global approaches, existing
neural network architecture such as GANs and VAEs are adapted to graph domain by
replacing CNN layers with GNN layers, albeit, with certain differences in
architectural choices. Notable works in this field include MolGAN
\cite{decaoMolGANImplicitGenerative2018}, GraphVAE
\cite{simonovskyGraphVAEGenerationSmall2018} and NetGAN
\cite{bojchevskiNetGANGeneratingGraphs2018}.


An exciting path of research in generative modeling is to adapt
\textit{normalizing flows models} in discrete domain, which has experienced a
surge in popularity due to the expressive power of discrete objects such as
graphs and sets. Normalizing flows models aim to learn a continuous invertible
and deterministic mapping between the data distribution and a latent
distribution, which is usually a Gaussian that is easy to manipulate and sample
from. Sampling can easily be done by sampling from the latent distribution and
run through the reverse mapping to retrieve a sample from the data distribution.
The mappings here are simply continuous invertible functions or a composition of
them. Designing these functions is the main area of focus when improving the
generalization power of these model
\cite{kobyzevNormalizingFlowsIntroduction2021}. Notable works in apply
normalizing flows to graph are GraphNVP
\cite{madhawaGraphNVPInvertibleFlow2019}, GraphAF
\cite{shiGraphAFFlowbasedAutoregressive2020} and MoFlow
\cite{zangMoFlowInvertibleFlow2020}. A recent development in normalizing flows
model are \textit{continuous flows}, where rather than a discrete sequence of
functions that are composed together, the invertible mapping is modeled as a
solution to a differential equation. Continuous time-dependent mappings allow
data distribution to smoothly "flow" toward the latent distribution, albeit,
require slightly more computational power to approximate the solution. One of
the most notable works in the domain of continuous flow is FFJORD
\cite{grathwohlFFJORDFreeformContinuous2018}. Application of continuous flows,
however, is a largely unexplored domain.

\section{Thesis outline}

This thesis aim to provide a systematic review of normalizing flows models for
graph generation, with a focus on graph neural networks as the primary
computational modules. The thesis is structured with concept dependencies in
mind, where later topics were written with the assumption that previous concepts
had already been read. Therefore, it is recommended that the thesis should be
read linearly, however, it is not a mandatory requirement for a pleasant read.

The thesis contains seven chapters, starting with Chapter \ref{c:bg}, where a
general overview of notations and related works on graph generation was given.
Chapter \ref{c:gnn} primary focus on graphs neural networks, explored under a
similar categorization framework as introduced by
\cite{zhouGraphNeuralNetworks2020}, however are compressed and filtered in terms
of relevance to normalizing flows. Chapter \ref{c:nf} considers a brief
walkthrough of the normalizing flows framework, including motivation,
formulation, categorization of different flows and a comparison with others
generative modeling architectures such as GANs
\cite{goodfellowGenerativeAdversarialNetworks2014} and VAEs
\cite{kingmaAutoEncodingVariationalBayes2014}. Chapter \ref{c:gnf} consists of
different application of normalizing flows on graphs data, with a particular
focus on molecule generation due to its high practical values and conveniently
being extensively studied in the literature. As for closing remarks, Chapter
\ref{c:dis} give an open discussion on graph generative models along with
personal thoughts on future works and research direction. Chapter
\ref{c:conclude} provide a final conclusion and closing remarks on the thesis.


\chapter{Background}
\label{c:bg}
\section{Notations}
\section{Related works}

\chapter{Graph Neural Network}
\label{c:gnn}

Graph neural networks (GNNs) in their most general definition can be considered
as deep learning based methods that directly operates on graphs. There have been
multiple surveys published in effort of standardizing and categorizing graph
neural networks: \cite{zhangdeeplearninggraphs2020}
\cite{zhangdeeplearninggraphs2020} \cite{chamiMachineLearningGraphs2022} are
notable mentions. However, GNN is a field of active research and its taxonomy
can change drastically in the future. For convenience and consistency, this
thesis adopts the categorization framework by \cite{zhangdeeplearninggraphs2020}
where graph neural networks are perceived through the lens of deep learning
architectural design. Under this view, graph neural networks pipeline are
composed of primarily \textit{computational modules} falling into three main
categories: propagation modules, sampling modules and pooling modules.
Propagation modules can be considered as the central information aggregators
that through the process of propagating information between nodes, are capable
of capturing both feature and topological information. There are two main
operators used in propagation: convolution operators and recurrent operators,
which can be generalized to an encompassing superset of convolutional graph
neural network (ConvGNNs) and recurrent graph neural networks (RecGNNs) from the
taxonomy proposed by \cite{wuComprehensiveSurveyGraph2021}. The central idea of
convolution operators is to generalize convolutions from the other domain to
graph domain. Convolutions abstractly can be represented as layers with
different parameters or weights that "convolve" with the information passed in
as inputs and then output a latent representation. The main distinction between
recurrent approaches and convolution approaches is the reusing of weights during
information propagation. While convolution use separate weights for each
operating layers, recurrent approaches recycle the layer and pass the output as
the new input and hence reuse the weights. As the result, the notion of layer
depth are mostly applied to convolution methods and not recurrent ones.

In an effort of scoping the field of GNN for application in normalizing flows
models, only convolutional graph neural networks are explored in detail in this
chapter. There are two main methods for convolution operators: spectral methods
and spatial methods.

\section{Spectral methods}
\section{Spatial methods}
\subsection{Basic approaches}
\subsection{Attention mechanism}
\subsection{Framework}

\chapter{Normalizing Flows}
\label{c:nf}
\section{Background}
\section{Linear Flows}
\section{Planar and Radial Flows}
\section{Coupling Flows}
\section{Autoregressive Flows}
\section{Residual Flows}
\section{Continuous Flows}

\chapter{Normalizing Flows for Graph Generation}
\label{c:gnf}
\section{GraphNVP}
\section{GraphAF}
\section{MoFlow}
\section{GraphDF}

\chapter{Discussion}
\label{c:dis}
\section{Future work}

\chapter{Conclusion}
\label{c:conclude}

%%  LIITTEET  ------------------------------------------

\appendix

\bibliography{ref.bib}
\bibliographystyle{apalike}

\end{document}
