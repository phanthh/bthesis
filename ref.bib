
@book{arnoldOrdinaryDifferentialEquations1992,
  title = {Ordinary {{Differential Equations}}},
  author = {Arnold, Vladimir I.},
  year = {1992},
  month = may,
  publisher = {{Springer Science \& Business Media}},
  abstract = {The first two chapters of this book have been thoroughly revised and sig nificantly expanded. Sections have been added on elementary methods of in tegration (on homogeneous and inhomogeneous first-order linear equations and on homogeneous and quasi-homogeneous equations), on first-order linear and quasi-linear partial differential equations, on equations not solved for the derivative, and on Sturm's theorems on the zeros of second-order linear equa tions. Thus the new edition contains all the questions of the current syllabus in the theory of ordinary differential equations. In discussing special devices for integration the author has tried through out to lay bare the geometric essence of the methods being studied and to show how these methods work in applications, especially in mechanics. Thus to solve an inhomogeneous linear equation we introduce the delta-function and calculate the retarded Green's function; quasi-homogeneous equations lead to the theory of similarity and the law of universal gravitation, while the theorem on differentiability of the solution with respect to the initial conditions leads to the study of the relative motion of celestial bodies in neighboring orbits. The author has permitted himself to include some historical digressions in this preface. Differential equations were invented by Newton (1642-1727).},
  googlebooks = {JUoyqlW7PZgC},
  isbn = {978-3-540-54813-3},
  langid = {english},
  keywords = {Mathematics / Calculus,Mathematics / Mathematical Analysis,Science / Physics / Mathematical \& Computational}
}

@article{asifGraphNeuralNetwork2021,
  title = {Graph {{Neural Network}}: {{A Comprehensive Review}} on {{Non-Euclidean Space}}},
  shorttitle = {Graph {{Neural Network}}},
  author = {Asif, Nurul A. and Sarker, Yeahia and Chakrabortty, Ripon K. and Ryan, Michael J. and Ahamed, Md. Hafiz and Saha, Dip K. and Badal, Faisal R. and Das, Sajal K. and Ali, Md. Firoz and Moyeen, Sumaya I. and Islam, Md. Robiul and Tasneem, Zinat},
  year = {2021},
  journal = {IEEE Access},
  volume = {9},
  pages = {60588--60606},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3071274},
  abstract = {This review provides a comprehensive overview of the state-of-the-art methods of graph-based networks from a deep learning perspective. Graph networks provide a generalized form to exploit non-euclidean space data. A graph can be visualized as an aggregation of nodes and edges without having any order. Data-driven architecture tends to follow a fixed neural network trying to find the pattern in feature space. These strategies have successfully been applied to many applications for euclidean space data. Since graph data in a non-euclidean space does not follow any kind of order, these solutions can be applied to exploit the node relationships. Graph Neural Networks (GNNs) solve this problem by exploiting the relationships among graph data. Recent developments in computational hardware and optimization allow graph networks possible to learn the complex graph relationships. Graph networks are therefore being actively used to solve many problems including protein interface, classification, and learning representations of fingerprints. To encapsulate the importance of graph models, in this paper, we formulate a systematic categorization of GNN models according to their applications from theory to real-life problems and provide a direction of the future scope for the applications of graph models as well as highlight the limitations of existing graph networks.},
  keywords = {Computational modeling,Convolution,Feature extraction,geometric deep learning,Graph neural network,Graph neural networks,graph-structured network,Licenses,non-euclidean space,Task analysis,Taxonomy},
  file = {/Users/phanthh/Zotero/storage/M697LECL/Asif et al_2021_Graph Neural Network.pdf}
}

@misc{bojchevskiNetGANGeneratingGraphs2018,
  title = {{{NetGAN}}: {{Generating Graphs}} via {{Random Walks}}},
  shorttitle = {{{NetGAN}}},
  author = {Bojchevski, Aleksandar and Shchur, Oleksandr and Z{\"u}gner, Daniel and G{\"u}nnemann, Stephan},
  year = {2018},
  month = jun,
  number = {arXiv:1803.00816},
  eprint = {1803.00816},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1803.00816},
  abstract = {We propose NetGAN - the first implicit generative model for graphs able to mimic real-world networks. We pose the problem of graph generation as learning the distribution of biased random walks over the input graph. The proposed model is based on a stochastic neural network that generates discrete output samples and is trained using the Wasserstein GAN objective. NetGAN is able to produce graphs that exhibit well-known network patterns without explicitly specifying them in the model definition. At the same time, our model exhibits strong generalization properties, as highlighted by its competitive link prediction performance, despite not being trained specifically for this task. Being the first approach to combine both of these desirable properties, NetGAN opens exciting avenues for further research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Social and Information Networks,Statistics - Machine Learning},
  file = {/Users/phanthh/Zotero/storage/STTHMN9F/Bojchevski et al_2018_NetGAN.pdf}
}

@article{bronsteinGeometricDeepLearning2017,
  title = {Geometric {{Deep Learning}}: {{Going}} beyond {{Euclidean}} Data},
  shorttitle = {Geometric {{Deep Learning}}},
  author = {Bronstein, Michael M. and Bruna, Joan and LeCun, Yann and Szlam, Arthur and Vandergheynst, Pierre},
  year = {2017},
  month = jul,
  journal = {IEEE Signal Processing Magazine},
  volume = {34},
  number = {4},
  pages = {18--42},
  issn = {1558-0792},
  doi = {10.1109/MSP.2017.2693418},
  abstract = {Geometric deep learning is an umbrella term for emerging techniques attempting to generalize (structured) deep neural models to non-Euclidean domains, such as graphs and manifolds. The purpose of this article is to overview different examples of geometric deep-learning problems and present available solutions, key difficulties, applications, and future research directions in this nascent field.},
  keywords = {Computational modeling,Computer architecture,Convolution,Convolutional codes,Euclidean distance,Machine learning,Social network services},
  file = {/Users/phanthh/Zotero/storage/B2DG57PY/Bronstein et al. - 2017 - Geometric Deep Learning Going beyond Euclidean da.pdf;/Users/phanthh/Zotero/storage/DRWDSPM3/Bronstein et al_2017_Geometric Deep Learning.pdf}
}

@misc{bronsteinGeometricDeepLearning2021,
  title = {Geometric {{Deep Learning}}: {{Grids}}, {{Groups}}, {{Graphs}}, {{Geodesics}}, and {{Gauges}}},
  shorttitle = {Geometric {{Deep Learning}}},
  author = {Bronstein, Michael M. and Bruna, Joan and Cohen, Taco and Veli{\v c}kovi{\'c}, Petar},
  year = {2021},
  month = may,
  number = {arXiv:2104.13478},
  eprint = {2104.13478},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2104.13478},
  abstract = {The last decade has witnessed an experimental revolution in data science and machine learning, epitomised by deep learning methods. Indeed, many high-dimensional learning tasks previously thought to be beyond reach -- such as computer vision, playing Go, or protein folding -- are in fact feasible with appropriate computational scale. Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent type methods, typically implemented as backpropagation. While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world. This text is concerned with exposing these regularities through unified geometric principles that can be applied throughout a wide spectrum of applications. Such a 'geometric unification' endeavour, in the spirit of Felix Klein's Erlangen Program, serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other hand, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computational Geometry,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/phanthh/Zotero/storage/MRWCEFZG/Bronstein et al_2021_Geometric Deep Learning.pdf;/Users/phanthh/Zotero/storage/D2GH32XM/2104.html}
}

@misc{chamiMachineLearningGraphs2022,
  title = {Machine {{Learning}} on {{Graphs}}: {{A Model}} and {{Comprehensive Taxonomy}}},
  shorttitle = {Machine {{Learning}} on {{Graphs}}},
  author = {Chami, Ines and {Abu-El-Haija}, Sami and Perozzi, Bryan and R{\'e}, Christopher and Murphy, Kevin},
  year = {2022},
  month = apr,
  number = {arXiv:2005.03675},
  eprint = {2005.03675},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2005.03675},
  abstract = {There has been a surge of recent interest in learning representations for graph-structured data. Graph representation learning methods have generally fallen into three main categories, based on the availability of labeled data. The first, network embedding (such as shallow graph embedding or graph auto-encoders), focuses on learning unsupervised representations of relational structure. The second, graph regularized neural networks, leverages graphs to augment neural network losses with a regularization objective for semi-supervised learning. The third, graph neural networks, aims to learn differentiable functions over discrete topologies with arbitrary structure. However, despite the popularity of these areas there has been surprisingly little work on unifying the three paradigms. Here, we aim to bridge the gap between graph neural networks, network embedding and graph regularization models. We propose a comprehensive taxonomy of representation learning methods for graph-structured data, aiming to unify several disparate bodies of work. Specifically, we propose a Graph Encoder Decoder Model (GRAPHEDM), which generalizes popular algorithms for semi-supervised learning on graphs (e.g. GraphSage, Graph Convolutional Networks, Graph Attention Networks), and unsupervised learning of graph representations (e.g. DeepWalk, node2vec, etc) into a single consistent approach. To illustrate the generality of this approach, we fit over thirty existing methods into this framework. We believe that this unifying view both provides a solid foundation for understanding the intuition behind these methods, and enables future research in the area.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Social and Information Networks,Statistics - Machine Learning},
  file = {/Users/phanthh/Zotero/storage/4BVIE5WX/Chami et al_2022_Machine Learning on Graphs.pdf}
}

@article{chengMolecularDesignDrug2021,
  title = {Molecular Design in Drug Discovery: A Comprehensive Review of Deep Generative Models},
  shorttitle = {Molecular Design in Drug Discovery},
  author = {Cheng, Yu and Gong, Yongshun and Liu, Yuansheng and Song, Bosheng and Zou, Quan},
  year = {2021},
  month = nov,
  journal = {Briefings in Bioinformatics},
  volume = {22},
  number = {6},
  pages = {bbab344},
  issn = {1477-4054},
  doi = {10.1093/bib/bbab344},
  abstract = {Deep generative models have been an upsurge in the deep learning community since they were proposed. These models are designed for generating new synthetic data including images, videos and texts by fitting the data approximate distributions. In the last few years, deep generative models have shown superior performance in drug discovery especially de novo molecular design. In this study, deep generative models are reviewed to witness the recent advances of de novo molecular design for drug discovery. In addition, we divide those models into two categories based on molecular representations in silico. Then these two classical types of models are reported in detail and discussed about both pros and cons. We also indicate the current challenges in deep generative models for de novo molecular design. De novo molecular design automatically is promising but a long road to be explored.},
  file = {/Users/phanthh/Zotero/storage/LGLDFPJ6/6355420.html}
}

@misc{chenNeuralOrdinaryDifferential2019,
  title = {Neural {{Ordinary Differential Equations}}},
  author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
  year = {2019},
  month = dec,
  number = {arXiv:1806.07366},
  eprint = {1806.07366},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/phanthh/Zotero/storage/8JHSBS2P/Chen et al_2019_Neural Ordinary Differential Equations.pdf;/Users/phanthh/Zotero/storage/6UCCGARK/1806.html}
}

@misc{choromanskiRethinkingAttentionPerformers2021,
  title = {Rethinking {{Attention}} with {{Performers}}},
  author = {Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and Belanger, David and Colwell, Lucy and Weller, Adrian},
  year = {2021},
  month = mar,
  number = {arXiv:2009.14794},
  eprint = {2009.14794},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2009.14794},
  abstract = {We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can be also used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/phanthh/Zotero/storage/M23FCSF9/Choromanski et al_2021_Rethinking Attention with Performers.pdf}
}

@inproceedings{daiSyntaxdirectedVariationalAutoencoder2018,
  title = {Syntax-Directed Variational Autoencoder for Molecule Generation},
  booktitle = {Proceedings of the International Conference on Learning Representations},
  author = {Dai, Hanjun and Tian, Yingtao and Dai, Bo and Skiena, Steven and Song, Le},
  year = {2018},
  file = {/Users/phanthh/Zotero/storage/4NF7LKFK/Dai et al_2018_Syntax-directed variational autoencoder for molecule generation.pdf}
}

@misc{decaoMolGANImplicitGenerative2018,
  title = {{{MolGAN}}: {{An}} Implicit Generative Model for Small Molecular Graphs},
  shorttitle = {{{MolGAN}}},
  author = {De Cao, Nicola and Kipf, Thomas},
  year = {2018},
  month = may,
  number = {arXiv:1805.11973},
  eprint = {1805.11973},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1805.11973},
  abstract = {Deep generative models for graph-structured data offer a new angle on the problem of chemical synthesis: by optimizing differentiable models that directly generate molecular graphs, it is possible to side-step expensive search procedures in the discrete and vast space of chemical structures. We introduce MolGAN, an implicit, likelihood-free generative model for small molecular graphs that circumvents the need for expensive graph matching procedures or node ordering heuristics of previous likelihood-based methods. Our method adapts generative adversarial networks (GANs) to operate directly on graph-structured data. We combine our approach with a reinforcement learning objective to encourage the generation of molecules with specific desired chemical properties. In experiments on the QM9 chemical database, we demonstrate that our model is capable of generating close to 100\% valid compounds. MolGAN compares favorably both to recent proposals that use string-based (SMILES) representations of molecules and to a likelihood-based method that directly generates graphs, albeit being susceptible to mode collapse.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/phanthh/Zotero/storage/G2YUUE2E/De Cao_Kipf_2018_MolGAN.pdf}
}

@misc{dinhDensityEstimationUsing2017,
  title = {Density Estimation Using {{Real NVP}}},
  author = {Dinh, Laurent and {Sohl-Dickstein}, Jascha and Bengio, Samy},
  year = {2017},
  month = feb,
  number = {arXiv:1605.08803},
  eprint = {1605.08803},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1605.08803},
  abstract = {Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/phanthh/Zotero/storage/SP2SP2YF/Dinh et al_2017_Density estimation using Real NVP.pdf;/Users/phanthh/Zotero/storage/ZM662CXM/1605.html}
}

@misc{dinhNICENonlinearIndependent2015,
  title = {{{NICE}}: {{Non-linear Independent Components Estimation}}},
  shorttitle = {{{NICE}}},
  author = {Dinh, Laurent and Krueger, David and Bengio, Yoshua},
  year = {2015},
  month = apr,
  number = {arXiv:1410.8516},
  eprint = {1410.8516},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1410.8516},
  abstract = {We propose a deep learning framework for modeling complex high-dimensional densities called Non-linear Independent Component Estimation (NICE). It is based on the idea that a good representation is one in which the data has a distribution that is easy to model. For this purpose, a non-linear deterministic transformation of the data is learned that maps it to a latent space so as to make the transformed data conform to a factorized distribution, i.e., resulting in independent latent variables. We parametrize this transformation so that computing the Jacobian determinant and inverse transform is trivial, yet we maintain the ability to learn complex non-linear transformations, via a composition of simple building blocks, each based on a deep neural network. The training criterion is simply the exact log-likelihood, which is tractable. Unbiased ancestral sampling is also easy. We show that this approach yields good generative models on four image datasets and can be used for inpainting.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/phanthh/Zotero/storage/2EM7AA27/Dinh et al_2015_NICE.pdf;/Users/phanthh/Zotero/storage/5YZ5GZMR/1410.html}
}

@misc{duMolGenSurveySystematicSurvey2022,
  title = {{{MolGenSurvey}}: {{A Systematic Survey}} in {{Machine Learning Models}} for {{Molecule Design}}},
  shorttitle = {{{MolGenSurvey}}},
  author = {Du, Yuanqi and Fu, Tianfan and Sun, Jimeng and Liu, Shengchao},
  year = {2022},
  month = mar,
  number = {arXiv:2203.14500},
  eprint = {2203.14500},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.14500},
  abstract = {Molecule design is a fundamental problem in molecular science and has critical applications in a variety of areas, such as drug discovery, material science, etc. However, due to the large searching space, it is impossible for human experts to enumerate and test all molecules in wet-lab experiments. Recently, with the rapid development of machine learning methods, especially generative methods, molecule design has achieved great progress by leveraging machine learning models to generate candidate molecules. In this paper, we systematically review the most relevant work in machine learning models for molecule design. We start with a brief review of the mainstream molecule featurization and representation methods (including 1D string, 2D graph, and 3D geometry) and general generative methods (deep generative and combinatorial optimization methods). Then we summarize all the existing molecule design problems into several venues according to the problem setup, including input, output types and goals. Finally, we conclude with the open challenges and point out future opportunities of machine learning models for molecule design in real-world applications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computational Engineering; Finance; and Science,Computer Science - Machine Learning,Quantitative Biology - Biomolecules},
  file = {/Users/phanthh/Zotero/storage/EMZGBMQX/Du et al_2022_MolGenSurvey.pdf;/Users/phanthh/Zotero/storage/AZ35ZPRB/2203.html}
}

@misc{dupontAugmentedNeuralODEs2019,
  title = {Augmented {{Neural ODEs}}},
  author = {Dupont, Emilien and Doucet, Arnaud and Teh, Yee Whye},
  year = {2019},
  month = oct,
  number = {arXiv:1904.01681},
  eprint = {1904.01681},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1904.01681},
  abstract = {We show that Neural Ordinary Differential Equations (ODEs) learn representations that preserve the topology of the input space and prove that this implies the existence of functions Neural ODEs cannot represent. To address these limitations, we introduce Augmented Neural ODEs which, in addition to being more expressive models, are empirically more stable, generalize better and have a lower computational cost than Neural ODEs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/phanthh/Zotero/storage/W2QVA8P2/Dupont et al_2019_Augmented Neural ODEs.pdf;/Users/phanthh/Zotero/storage/A3SR9PS5/1904.html}
}

@misc{durkanCubicSplineFlows2019,
  title = {Cubic-{{Spline Flows}}},
  author = {Durkan, Conor and Bekasov, Artur and Murray, Iain and Papamakarios, George},
  year = {2019},
  month = jun,
  number = {arXiv:1906.02145},
  eprint = {1906.02145},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1906.02145},
  abstract = {A normalizing flow models a complex probability density as an invertible transformation of a simple density. The invertibility means that we can evaluate densities and generate samples from a flow. In practice, autoregressive flow-based models are slow to invert, making either density estimation or sample generation slow. Flows based on coupling transforms are fast for both tasks, but have previously performed less well at density estimation than autoregressive flows. We stack a new coupling transform, based on monotonic cubic splines, with LU-decomposed linear layers. The resulting cubic-spline flow retains an exact one-pass inverse, can be used to generate high-quality images, and closes the gap with autoregressive flows on a suite of density-estimation tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/phanthh/Zotero/storage/I53GLGBJ/Durkan et al_2019_Cubic-Spline Flows.pdf;/Users/phanthh/Zotero/storage/CC2X5EYZ/1906.html}
}

@misc{finlayHowTrainYour2020,
  title = {How to Train Your Neural {{ODE}}: The World of {{Jacobian}} and Kinetic Regularization},
  shorttitle = {How to Train Your Neural {{ODE}}},
  author = {Finlay, Chris and Jacobsen, J{\"o}rn-Henrik and Nurbekyan, Levon and Oberman, Adam M.},
  year = {2020},
  month = jun,
  number = {arXiv:2002.02798},
  eprint = {2002.02798},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2002.02798},
  abstract = {Training neural ODEs on large datasets has not been tractable due to the necessity of allowing the adaptive numerical ODE solver to refine its step size to very small values. In practice this leads to dynamics equivalent to many hundreds or even thousands of layers. In this paper, we overcome this apparent difficulty by introducing a theoretically-grounded combination of both optimal transport and stability regularizations which encourage neural ODEs to prefer simpler dynamics out of all the dynamics that solve a problem well. Simpler dynamics lead to faster convergence and to fewer discretizations of the solver, considerably decreasing wall-clock time without loss in performance. Our approach allows us to train neural ODE-based generative models to the same performance as the unregularized dynamics, with significant reductions in training time. This brings neural ODEs closer to practical relevance in large-scale applications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/phanthh/Zotero/storage/XR3VL7BE/Finlay et al_2020_How to train your neural ODE.pdf;/Users/phanthh/Zotero/storage/B6KNJGAT/2002.html}
}

@inproceedings{gilmerNeuralMessagePassing2017,
  title = {Neural {{Message Passing}} for {{Quantum Chemistry}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Gilmer, Justin and Schoenholz, Samuel S. and Riley, Patrick F. and Vinyals, Oriol and Dahl, George E.},
  year = {2017},
  month = jul,
  pages = {1263--1272},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.},
  langid = {english},
  file = {/Users/phanthh/Zotero/storage/L4UQ4H7V/Gilmer et al_2017_Neural Message Passing for Quantum Chemistry.pdf;/Users/phanthh/Zotero/storage/YGUJTGDF/Gilmer et al. - 2017 - Neural Message Passing for Quantum Chemistry.pdf}
}

@article{gomez-bombarelliAutomaticChemicalDesign2018,
  title = {Automatic {{Chemical Design Using}} a {{Data-Driven Continuous Representation}} of {{Molecules}}},
  author = {{G{\'o}mez-Bombarelli}, Rafael and Wei, Jennifer N. and Duvenaud, David and {Hern{\'a}ndez-Lobato}, Jos{\'e} Miguel and {S{\'a}nchez-Lengeling}, Benjam{\'i}n and Sheberla, Dennis and {Aguilera-Iparraguirre}, Jorge and Hirzel, Timothy D. and Adams, Ryan P. and {Aspuru-Guzik}, Al{\'a}n},
  year = {2018},
  month = feb,
  journal = {ACS Central Science},
  volume = {4},
  number = {2},
  pages = {268--276},
  publisher = {{American Chemical Society}},
  issn = {2374-7943},
  doi = {10.1021/acscentsci.7b00572},
  abstract = {We report a method to convert discrete representations of molecules to and from a multidimensional continuous representation. This model allows us to generate new molecules for efficient exploration and optimization through open-ended spaces of chemical compounds. A deep neural network was trained on hundreds of thousands of existing chemical structures to construct three coupled functions: an encoder, a decoder, and a predictor. The encoder converts the discrete representation of a molecule into a real-valued continuous vector, and the decoder converts these continuous vectors back to discrete molecular representations. The predictor estimates chemical properties from the latent continuous vector representation of the molecule. Continuous representations of molecules allow us to automatically generate novel chemical structures by performing simple operations in the latent space, such as decoding random vectors, perturbing known chemical structures, or interpolating between molecules. Continuous representations also allow the use of powerful gradient-based optimization to efficiently guide the search for optimized functional compounds. We demonstrate our method in the domain of drug-like molecules and also in a set of molecules with fewer that nine heavy atoms.},
  file = {/Users/phanthh/Zotero/storage/GD8YKB2L/Gómez-Bombarelli et al_2018_Automatic Chemical Design Using a Data-Driven Continuous Representation of.pdf;/Users/phanthh/Zotero/storage/GZJ6KSGB/Gómez-Bombarelli et al. - 2018 - Automatic Chemical Design Using a Data-Driven Cont.pdf}
}

@misc{goodfellowGenerativeAdversarialNetworks2014,
  title = {Generative {{Adversarial Networks}}},
  author = {Goodfellow, Ian J. and {Pouget-Abadie}, Jean and Mirza, Mehdi and Xu, Bing and {Warde-Farley}, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  year = {2014},
  month = jun,
  number = {arXiv:1406.2661},
  eprint = {1406.2661},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1406.2661},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/phanthh/Zotero/storage/P2MLS8NH/Goodfellow et al_2014_Generative Adversarial Networks.pdf;/Users/phanthh/Zotero/storage/A8Q58MUI/1406.html}
}

@misc{grathwohlFFJORDFreeformContinuous2018,
  title = {{{FFJORD}}: {{Free-form Continuous Dynamics}} for {{Scalable Reversible Generative Models}}},
  shorttitle = {{{FFJORD}}},
  author = {Grathwohl, Will and Chen, Ricky T. Q. and Bettencourt, Jesse and Sutskever, Ilya and Duvenaud, David},
  year = {2018},
  month = oct,
  number = {arXiv:1810.01367},
  eprint = {1810.01367},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1810.01367},
  abstract = {A promising class of generative models maps points from a simple distribution to a complex distribution through an invertible neural network. Likelihood-based training of these models requires restricting their architectures to allow cheap computation of Jacobian determinants. Alternatively, the Jacobian trace can be used if the transformation is specified by an ordinary differential equation. In this paper, we use Hutchinson's trace estimator to give a scalable unbiased estimate of the log-density. The result is a continuous-time invertible generative model with unbiased density estimation and one-pass sampling, while allowing unrestricted neural network architectures. We demonstrate our approach on high-dimensional density estimation, image generation, and variational inference, achieving the state-of-the-art among exact likelihood methods with efficient sampling.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/phanthh/Zotero/storage/TM9KHRUU/Grathwohl et al_2018_FFJORD.pdf}
}

@inproceedings{grcicDenselyConnectedNormalizing2021,
  title = {Densely Connected Normalizing Flows},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Grci{\'c}, Matej and Grubi{\v s}i{\'c}, Ivan and {\v S}egvi{\'c}, Sini{\v s}a},
  year = {2021},
  volume = {34},
  pages = {23968--23982},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Normalizing flows are bijective mappings between inputs and latent representations with a fully factorized distribution. They are very attractive due to exact likelihood evaluation and efficient sampling. However, their effective capacity is often insufficient since the bijectivity constraint limits the model width. We address this issue by incrementally padding intermediate representations with noise. We precondition the noise in accordance with previous invertible units, which we describe as cross-unit coupling. Our invertible glow-like modules increase the model expressivity by fusing a densely connected block with Nystr\"om self-attention. We refer to our architecture as DenseFlow since both cross-unit and intra-module couplings rely on dense connectivity. Experiments show significant improvements due to the proposed contributions and reveal state-of-the-art density estimation under moderate computing budgets.},
  file = {/Users/phanthh/Zotero/storage/3MW4V59I/Grcić et al_2021_Densely connected normalizing flows.pdf}
}

@article{haghighatlariAdvancesMachineLearning2019,
  title = {Advances of Machine Learning in Molecular Modeling and Simulation},
  author = {Haghighatlari, Mojtaba and Hachmann, Johannes},
  year = {2019},
  month = mar,
  journal = {Current Opinion in Chemical Engineering},
  series = {Frontiers of {{Chemical Engineering}}: {{Molecular Modeling}}},
  volume = {23},
  pages = {51--57},
  issn = {2211-3398},
  doi = {10.1016/j.coche.2019.02.009},
  abstract = {In this review, we highlight recent developments in the application of machine learning for molecular modeling and simulation. After giving a brief overview of the foundations, components, and workflow of a typical supervised learning approach for chemical problems, we showcase areas and state-of-the-art examples of their deployment. In this context, we discuss how machine learning relates to, supports, and augments more traditional physics-based approaches in computational research. We conclude by outlining challenges and future research directions that need to be addressed in order to make machine learning a mainstream chemical engineering tool.},
  langid = {english},
  file = {/Users/phanthh/Zotero/storage/NI4PAK6J/Haghighatlari_Hachmann_2019_Advances of machine learning in molecular modeling and simulation.pdf;/Users/phanthh/Zotero/storage/RCINUQ5A/S2211339818300832.html}
}

@inproceedings{heDeepResidualLearning2016,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2016},
  pages = {770--778},
  file = {/Users/phanthh/Zotero/storage/FC5UNJKS/He et al_2016_Deep Residual Learning for Image Recognition.pdf;/Users/phanthh/Zotero/storage/RF3WJVXK/He_Deep_Residual_Learning_CVPR_2016_paper.html}
}

@inproceedings{hersheyCNNArchitecturesLargescale2017,
  title = {{{CNN}} Architectures for Large-Scale Audio Classification},
  booktitle = {2017 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Hershey, Shawn and Chaudhuri, Sourish and Ellis, Daniel P. W. and Gemmeke, Jort F. and Jansen, Aren and Moore, R. Channing and Plakal, Manoj and Platt, Devin and Saurous, Rif A. and Seybold, Bryan and Slaney, Malcolm and Weiss, Ron J. and Wilson, Kevin},
  year = {2017},
  month = mar,
  pages = {131--135},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2017.7952132},
  abstract = {Convolutional Neural Networks (CNNs) have proven very effective in image classification and show promise for audio. We use various CNN architectures to classify the soundtracks of a dataset of 70M training videos (5.24 million hours) with 30,871 video-level labels. We examine fully connected Deep Neural Networks (DNNs), AlexNet [1], VGG [2], Inception [3], and ResNet [4]. We investigate varying the size of both training set and label vocabulary, finding that analogs of the CNNs used in image classification do well on our audio classification task, and larger training and label sets help up to a point. A model using embeddings from these classifiers does much better than raw features on the Audio Set [5] Acoustic Event Detection (AED) classification task.},
  keywords = {Acoustic Event Detection,Acoustic Scene Classification,Computer architecture,Convolutional Neural Networks,Deep Neural Networks,Hidden Markov models,Neural networks,Servers,Spectrogram,Training,Video Classification,Videos},
  file = {/Users/phanthh/Zotero/storage/53KSDEYW/Hershey et al. - 2017 - CNN architectures for large-scale audio classifica.pdf;/Users/phanthh/Zotero/storage/JRE67HMF/Hershey et al_2017_CNN architectures for large-scale audio classification.pdf}
}

@misc{hoFlowImprovingFlowBased2019,
  title = {Flow++: {{Improving Flow-Based Generative Models}} with {{Variational Dequantization}} and {{Architecture Design}}},
  shorttitle = {Flow++},
  author = {Ho, Jonathan and Chen, Xi and Srinivas, Aravind and Duan, Yan and Abbeel, Pieter},
  year = {2019},
  month = may,
  number = {arXiv:1902.00275},
  eprint = {1902.00275},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1902.00275},
  abstract = {Flow-based generative models are powerful exact likelihood models with efficient sampling and inference. Despite their computational efficiency, flow-based models generally have much worse density modeling performance compared to state-of-the-art autoregressive models. In this paper, we investigate and improve upon three limiting design choices employed by flow-based models in prior work: the use of uniform noise for dequantization, the use of inexpressive affine flows, and the use of purely convolutional conditioning networks in coupling layers. Based on our findings, we propose Flow++, a new flow-based model that is now the state-of-the-art non-autoregressive model for unconditional density estimation on standard image benchmarks. Our work has begun to close the significant performance gap that has so far existed between autoregressive models and flow-based models. Our implementation is available at https://github.com/aravindsrinivas/flowpp},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/phanthh/Zotero/storage/9V7UTKIC/Ho et al_2019_Flow++.pdf;/Users/phanthh/Zotero/storage/8CYIR34D/1902.html}
}

@misc{hondaGraphResidualFlow2019,
  title = {Graph {{Residual Flow}} for {{Molecular Graph Generation}}},
  author = {Honda, Shion and Akita, Hirotaka and Ishiguro, Katsuhiko and Nakanishi, Toshiki and Oono, Kenta},
  year = {2019},
  month = sep,
  number = {arXiv:1909.13521},
  eprint = {1909.13521},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1909.13521},
  abstract = {Statistical generative models for molecular graphs attract attention from many researchers from the fields of bio- and chemo-informatics. Among these models, invertible flow-based approaches are not fully explored yet. In this paper, we propose a powerful invertible flow for molecular graphs, called graph residual flow (GRF). The GRF is based on residual flows, which are known for more flexible and complex non-linear mappings than traditional coupling flows. We theoretically derive non-trivial conditions such that GRF is invertible, and present a way of keeping the entire flows invertible throughout the training and sampling. Experimental results show that a generative model based on the proposed GRF achieves comparable generation performance, with much smaller number of trainable parameters compared to the existing flow-based model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/phanthh/Zotero/storage/PB6D9F4Q/Honda et al_2019_Graph Residual Flow for Molecular Graph Generation.pdf}
}

@inproceedings{hoogeboomEmergingConvolutionsGenerative2019,
  title = {Emerging {{Convolutions}} for {{Generative Normalizing Flows}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Hoogeboom, Emiel and Berg, Rianne Van Den and Welling, Max},
  year = {2019},
  month = may,
  pages = {2771--2780},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Generative flows are attractive because they admit exact likelihood optimization and efficient image synthesis. Recently, Kingma \& Dhariwal (2018) demonstrated with Glow that generative flows are capable of generating high quality images. We generalize the 1 \{\textbackslash texttimes\} 1 convolutions proposed in Glow to invertible d \{\textbackslash texttimes\} d convolutions, which are more flexible since they operate on both channel and spatial axes. We propose two methods to produce invertible convolutions, that have receptive fields identical to standard convolutions: Emerging convolutions are obtained by chaining specific autoregressive convolutions, and periodic convolutions are decoupled in the frequency domain. Our experiments show that the flexibility of d \{\textbackslash texttimes\} d convolutions significantly improves the performance of generative flow models on galaxy images, CIFAR10 and ImageNet.},
  langid = {english},
  file = {/Users/phanthh/Zotero/storage/H3ZF8EHQ/Hoogeboom et al_2019_Emerging Convolutions for Generative Normalizing Flows.pdf}
}

@inproceedings{huangNeuralAutoregressiveFlows2018,
  title = {Neural {{Autoregressive Flows}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Huang, Chin-Wei and Krueger, David and Lacoste, Alexandre and Courville, Aaron},
  year = {2018},
  month = jul,
  pages = {2078--2087},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Normalizing flows and autoregressive models have been successfully combined to produce state-of-the-art results in density estimation, via Masked Autoregressive Flows (MAF) (Papamakarios et al., 2017), and to accelerate state-of-the-art WaveNet-based speech synthesis to 20x faster than real-time (Oord et al., 2017), via Inverse Autoregressive Flows (IAF) (Kingma et al., 2016). We unify and generalize these approaches, replacing the (conditionally) affine univariate transformations of MAF/IAF with a more general class of invertible univariate transformations expressed as monotonic neural networks. We demonstrate that the proposed neural autoregressive flows (NAF) are universal approximators for continuous probability distributions, and their greater expressivity allows them to better capture multimodal target distributions. Experimentally, NAF yields state-of-the-art performance on a suite of density estimation tasks and outperforms IAF in variational autoencoders trained on binarized MNIST.},
  langid = {english},
  file = {/Users/phanthh/Zotero/storage/ACDB69QZ/Huang et al_2018_Neural Autoregressive Flows.pdf;/Users/phanthh/Zotero/storage/V8V57XDW/Huang et al. - 2018 - Neural Autoregressive Flows.pdf}
}

@misc{jainiSumofSquaresPolynomialFlow2019,
  title = {Sum-of-{{Squares Polynomial Flow}}},
  author = {Jaini, Priyank and Selby, Kira A. and Yu, Yaoliang},
  year = {2019},
  month = jun,
  number = {arXiv:1905.02325},
  eprint = {1905.02325},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1905.02325},
  abstract = {Triangular map is a recent construct in probability theory that allows one to transform any source probability density function to any target density function. Based on triangular maps, we propose a general framework for high-dimensional density estimation, by specifying one-dimensional transformations (equivalently conditional densities) and appropriate conditioner networks. This framework (a) reveals the commonalities and differences of existing autoregressive and flow based methods, (b) allows a unified understanding of the limitations and representation power of these recent approaches and, (c) motivates us to uncover a new Sum-of-Squares (SOS) flow that is interpretable, universal, and easy to train. We perform several synthetic experiments on various density geometries to demonstrate the benefits (and short-comings) of such transformations. SOS flows achieve competitive results in simulations and several real-world datasets.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/phanthh/Zotero/storage/LDQUC2JE/Jaini et al_2019_Sum-of-Squares Polynomial Flow.pdf;/Users/phanthh/Zotero/storage/Y36JKLNW/1905.html}
}

@book{katokIntroductionModernTheory1997,
  title = {Introduction to the {{Modern Theory}} of {{Dynamical Systems}}},
  author = {Katok, Anatole and Hasselblatt, Boris},
  year = {1997},
  publisher = {{Cambridge University Press}},
  abstract = {This book provided the first self-contained comprehensive exposition of the theory of dynamical systems as a core mathematical discipline closely intertwined with most of the main areas of mathematics. The authors introduce and rigorously develop the theory while providing researchers interested in applications with fundamental tools and paradigms. The book begins with a discussion of several elementary but fundamental examples. These are used to formulate a program for the general study of asymptotic properties and to introduce the principal theoretical concepts and methods. The main theme of the second part of the book is the interplay between local analysis near individual orbits and the global complexity of the orbit structure. The third and fourth parts develop the theories of low-dimensional dynamical systems and hyperbolic dynamical systems in depth. Over 400 systematic exercises are included in the text. The book is aimed at students and researchers in mathematics at all levels from advanced undergraduate up.},
  googlebooks = {9nL7ZX8Djp4C},
  isbn = {978-0-521-57557-7},
  langid = {english},
  keywords = {Mathematics / Calculus,Mathematics / General,Mathematics / Probability \& Statistics / General}
}

@misc{kingmaAutoEncodingVariationalBayes2014,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2014},
  month = may,
  number = {arXiv:1312.6114},
  eprint = {1312.6114},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1312.6114},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/phanthh/Zotero/storage/BTR8JZVJ/Kingma_Welling_2014_Auto-Encoding Variational Bayes.pdf}
}

@inproceedings{kingmaGlowGenerativeFlow2018,
  title = {Glow: {{Generative Flow}} with {{Invertible}} 1x1 {{Convolutions}}},
  shorttitle = {Glow},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Kingma, Durk P and Dhariwal, Prafulla},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Flow-based generative models are conceptually attractive due to tractability of the exact log-likelihood, tractability of exact latent-variable inference, and parallelizability of both training and synthesis. In this paper we propose Glow, a simple type of generative flow using invertible 1x1 convolution. Using our method we demonstrate a significant improvement in log-likelihood and qualitative sample quality. Perhaps most strikingly, we demonstrate that a generative model optimized towards the plain log-likelihood objective is capable of efficient synthesis of large and subjectively realistic-looking images.},
  file = {/Users/phanthh/Zotero/storage/6QVA8KGA/Kingma_Dhariwal_2018_Glow.pdf}
}

@inproceedings{kingmaImprovedVariationalInference2016,
  title = {Improved {{Variational Inference}} with {{Inverse Autoregressive Flow}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Kingma, Durk P and Salimans, Tim and Jozefowicz, Rafal and Chen, Xi and Sutskever, Ilya and Welling, Max},
  year = {2016},
  volume = {29},
  publisher = {{Curran Associates, Inc.}},
  abstract = {The framework of normalizing flows provides a general strategy for flexible variational inference of posteriors over latent variables. We propose a new type of normalizing flow, inverse autoregressive flow (IAF), that, in contrast to earlier published flows, scales well to high-dimensional latent spaces. The proposed flow consists of a chain of invertible transformations, where each transformation is based on an autoregressive neural network. In experiments, we show that IAF significantly improves upon diagonal Gaussian approximate posteriors. In addition, we demonstrate that a novel type of variational autoencoder, coupled with IAF, is competitive with neural autoregressive models in terms of attained log-likelihood on natural images, while allowing significantly faster synthesis.},
  file = {/Users/phanthh/Zotero/storage/3TAYNRVM/Kingma et al_2016_Improved Variational Inference with Inverse Autoregressive Flow.pdf}
}

@misc{kipfSemiSupervisedClassificationGraph2017,
  title = {Semi-{{Supervised Classification}} with {{Graph Convolutional Networks}}},
  author = {Kipf, Thomas N. and Welling, Max},
  year = {2017},
  month = feb,
  number = {arXiv:1609.02907},
  eprint = {1609.02907},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1609.02907},
  abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/phanthh/Zotero/storage/5PRSJFQ4/Kipf_Welling_2017_Semi-Supervised Classification with Graph Convolutional Networks.pdf}
}

@article{kobyzevNormalizingFlowsIntroduction2021,
  title = {Normalizing {{Flows}}: {{An Introduction}} and {{Review}} of {{Current Methods}}},
  shorttitle = {Normalizing {{Flows}}},
  author = {Kobyzev, Ivan and Prince, Simon J. D. and Brubaker, Marcus A.},
  year = {2021},
  month = nov,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {43},
  number = {11},
  eprint = {1908.09257},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  pages = {3964--3979},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2020.2992934},
  abstract = {Normalizing Flows are generative models which produce tractable distributions where both sampling and density evaluation can be efficient and exact. The goal of this survey article is to give a coherent and comprehensive review of the literature around the construction and use of Normalizing Flows for distribution learning. We aim to provide context and explanation of the models, review current state-of-the-art literature, and identify open questions and promising future directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/phanthh/Zotero/storage/GDABQCGX/Kobyzev et al_2021_Normalizing Flows.pdf}
}

@misc{kruseHINTHierarchicalInvertible2021,
  title = {{{HINT}}: {{Hierarchical Invertible Neural Transport}} for {{Density Estimation}} and {{Bayesian Inference}}},
  shorttitle = {{{HINT}}},
  author = {Kruse, Jakob and Detommaso, Gianluca and K{\"o}the, Ullrich and Scheichl, Robert},
  year = {2021},
  month = may,
  number = {arXiv:1905.10687},
  eprint = {1905.10687},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1905.10687},
  abstract = {Many recent invertible neural architectures are based on coupling block designs where variables are divided in two subsets which serve as inputs of an easily invertible (usually affine) triangular transformation. While such a transformation is invertible, its Jacobian is very sparse and thus may lack expressiveness. This work presents a simple remedy by noting that subdivision and (affine) coupling can be repeated recursively within the resulting subsets, leading to an efficiently invertible block with dense, triangular Jacobian. By formulating our recursive coupling scheme via a hierarchical architecture, HINT allows sampling from a joint distribution p(y,x) and the corresponding posterior p(x|y) using a single invertible network. We evaluate our method on some standard data sets and benchmark its full power for density estimation and Bayesian inference on a novel data set of 2D shapes in Fourier parameterization, which enables consistent visualization of samples for different dimensionalities.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/phanthh/Zotero/storage/LHEKWX3G/Kruse et al_2021_HINT.pdf;/Users/phanthh/Zotero/storage/L5JA5BY5/1905.html}
}

@inproceedings{kusnerGrammarVariationalAutoencoder2017,
  title = {Grammar {{Variational Autoencoder}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Kusner, Matt J. and Paige, Brooks and {Hern{\'a}ndez-Lobato}, Jos{\'e} Miguel},
  year = {2017},
  month = jul,
  pages = {1945--1954},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Deep generative models have been wildly successful at learning coherent latent representations for continuous data such as natural images, artwork, and audio. However, generative modeling of discrete data such as arithmetic expressions and molecular structures still poses significant challenges. Crucially, state-of-the-art methods often produce outputs that are not valid. We make the key observation that frequently, discrete data can be represented as a parse tree from a context-free grammar. We propose a variational autoencoder which directly encodes from and decodes to these parse trees, ensuring the generated outputs are always syntactically valid. Surprisingly, we show that not only does our model more often generate valid outputs, it also learns a more coherent latent space in which nearby points decode to similar discrete outputs. We demonstrate the effectiveness of our learned models by showing their improved performance in Bayesian optimization for symbolic regression and molecule generation.},
  langid = {english},
  file = {/Users/phanthh/Zotero/storage/BP8T6225/Kusner et al. - 2017 - Grammar Variational Autoencoder.pdf;/Users/phanthh/Zotero/storage/CGSYAQXL/Kusner et al_2017_Grammar Variational Autoencoder.pdf}
}

@misc{liLearningDeepGenerative2018,
  title = {Learning {{Deep Generative Models}} of {{Graphs}}},
  author = {Li, Yujia and Vinyals, Oriol and Dyer, Chris and Pascanu, Razvan and Battaglia, Peter},
  year = {2018},
  month = mar,
  number = {arXiv:1803.03324},
  eprint = {1803.03324},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1803.03324},
  abstract = {Graphs are fundamental data structures which concisely capture the relational structure in many important real-world domains, such as knowledge graphs, physical and social interactions, language, and chemistry. Here we introduce a powerful new approach for learning generative models over graphs, which can capture both their structure and attributes. Our approach uses graph neural networks to express probabilistic dependencies among a graph's nodes and edges, and can, in principle, learn distributions over any arbitrary graph. In a series of experiments our results show that once trained, our models can generate good quality samples of both synthetic graphs as well as real molecular graphs, both unconditionally and conditioned on data. Compared to baselines that do not use graph-structured representations, our models often perform far better. We also explore key challenges of learning generative models of graphs, such as how to handle symmetries and ordering of elements during the graph generation process, and offer possible solutions. Our work is the first and most general approach for learning generative models over arbitrary graphs, and opens new directions for moving away from restrictions of vector- and sequence-like knowledge representations, toward more expressive and flexible relational data structures.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/phanthh/Zotero/storage/5TGW2QMF/Li et al_2018_Learning Deep Generative Models of Graphs.pdf}
}

@misc{luoGraphDFDiscreteFlow2021,
  title = {{{GraphDF}}: {{A Discrete Flow Model}} for {{Molecular Graph Generation}}},
  shorttitle = {{{GraphDF}}},
  author = {Luo, Youzhi and Yan, Keqiang and Ji, Shuiwang},
  year = {2021},
  month = jun,
  number = {arXiv:2102.01189},
  eprint = {2102.01189},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2102.01189},
  abstract = {We consider the problem of molecular graph generation using deep models. While graphs are discrete, most existing methods use continuous latent variables, resulting in inaccurate modeling of discrete graph structures. In this work, we propose GraphDF, a novel discrete latent variable model for molecular graph generation based on normalizing flow methods. GraphDF uses invertible modulo shift transforms to map discrete latent variables to graph nodes and edges. We show that the use of discrete latent variables reduces computational costs and eliminates the negative effect of dequantization. Comprehensive experimental results show that GraphDF outperforms prior methods on random generation, property optimization, and constrained optimization tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/phanthh/Zotero/storage/ZZPYA25M/Luo et al_2021_GraphDF.pdf;/Users/phanthh/Zotero/storage/J9BR6DJE/2102.html}
}

@misc{madhawaGraphNVPInvertibleFlow2019,
  title = {{{GraphNVP}}: {{An Invertible Flow Model}} for {{Generating Molecular Graphs}}},
  shorttitle = {{{GraphNVP}}},
  author = {Madhawa, Kaushalya and Ishiguro, Katushiko and Nakago, Kosuke and Abe, Motoki},
  year = {2019},
  month = may,
  number = {arXiv:1905.11600},
  eprint = {1905.11600},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1905.11600},
  abstract = {We propose GraphNVP, the first invertible, normalizing flow-based molecular graph generation model. We decompose the generation of a graph into two steps: generation of (i) an adjacency tensor and (ii) node attributes. This decomposition yields the exact likelihood maximization on graph-structured data, combined with two novel reversible flows. We empirically demonstrate that our model efficiently generates valid molecular graphs with almost no duplicated molecules. In addition, we observe that the learned latent space can be used to generate molecules with desired chemical properties.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/phanthh/Zotero/storage/B9NQKAM8/Madhawa et al_2019_GraphNVP.pdf}
}

@misc{mullerNeuralImportanceSampling2019,
  title = {Neural {{Importance Sampling}}},
  author = {M{\"u}ller, Thomas and McWilliams, Brian and Rousselle, Fabrice and Gross, Markus and Nov{\'a}k, Jan},
  year = {2019},
  month = sep,
  number = {arXiv:1808.03856},
  eprint = {1808.03856},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1808.03856},
  abstract = {We propose to use deep neural networks for generating samples in Monte Carlo integration. Our work is based on non-linear independent components estimation (NICE), which we extend in numerous ways to improve performance and enable its application to integration problems. First, we introduce piecewise-polynomial coupling transforms that greatly increase the modeling power of individual coupling layers. Second, we propose to preprocess the inputs of neural networks using one-blob encoding, which stimulates localization of computation and improves inference. Third, we derive a gradient-descent-based optimization for the KL and the \$\textbackslash chi\^2\$ divergence for the specific application of Monte Carlo integration with unnormalized stochastic estimates of the target distribution. Our approach enables fast and accurate inference and efficient sample generation independently of the dimensionality of the integration domain. We show its benefits on generating natural images and in two applications to light-transport simulation: first, we demonstrate learning of joint path-sampling densities in the primary sample space and importance sampling of multi-dimensional path prefixes thereof. Second, we use our technique to extract conditional directional densities driven by the product of incident illumination and the BSDF in the rendering equation, and we leverage the densities for path guiding. In all applications, our approach yields on-par or higher performance than competing techniques at equal sample count.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Graphics,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/phanthh/Zotero/storage/W7JK8XXI/Müller et al_2019_Neural Importance Sampling.pdf;/Users/phanthh/Zotero/storage/YQ877UID/1808.html}
}

@inproceedings{papamakariosMaskedAutoregressiveFlow2017,
  title = {Masked {{Autoregressive Flow}} for {{Density Estimation}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Papamakarios, George and Pavlakou, Theo and Murray, Iain},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Autoregressive models are among the best performing neural density estimators. We describe an approach for increasing the flexibility of an autoregressive model, based on modelling the random numbers that the model uses internally when generating data. By constructing a stack of autoregressive models, each modelling the random numbers of the next model in the stack, we obtain a type of normalizing flow suitable for density estimation, which we call Masked Autoregressive Flow. This type of flow is closely related to Inverse Autoregressive Flow and is a generalization of Real NVP. Masked Autoregressive Flow achieves state-of-the-art performance in a range of general-purpose density estimation tasks.},
  file = {/Users/phanthh/Zotero/storage/3ZEL6GG2/Papamakarios et al_2017_Masked Autoregressive Flow for Density Estimation.pdf}
}

@misc{papamakariosNormalizingFlowsProbabilistic2021,
  title = {Normalizing {{Flows}} for {{Probabilistic Modeling}} and {{Inference}}},
  author = {Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
  year = {2021},
  month = apr,
  number = {arXiv:1912.02762},
  eprint = {1912.02762},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1912.02762},
  abstract = {Normalizing flows provide a general mechanism for defining expressive probability distributions, only requiring the specification of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing flows, ranging from improving their expressive power to expanding their application. We believe the field has now matured and is in need of a unified perspective. In this review, we attempt to provide such a perspective by describing flows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of flow design, and discuss foundational topics such as expressive power and computational trade-offs. We also broaden the conceptual framing of flows by relating them to more general probability transformations. Lastly, we summarize the use of flows for tasks such as generative modeling, approximate inference, and supervised learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/phanthh/Zotero/storage/M4V43TD8/Papamakarios et al_2021_Normalizing Flows for Probabilistic Modeling and Inference.pdf;/Users/phanthh/Zotero/storage/UPF3FMLK/1912.html}
}

@article{parrDiscreteContinuousBrain2018,
  title = {The {{Discrete}} and {{Continuous Brain}}: {{From Decisions}} to {{Movement}}\textemdash{{And Back Again}}},
  shorttitle = {The {{Discrete}} and {{Continuous Brain}}},
  author = {Parr, Thomas and Friston, Karl J.},
  year = {2018},
  month = sep,
  journal = {Neural Computation},
  volume = {30},
  number = {9},
  pages = {2319--2347},
  issn = {0899-7667},
  doi = {10.1162/neco_a_01102},
  abstract = {To act upon the world, creatures must change continuous variables such as muscle length or chemical concentration. In contrast, decision making is an inherently discrete process, involving the selection among alternative courses of action. In this article, we consider the interface between the discrete and continuous processes that translate our decisions into movement in a Newtonian world\textemdash and how movement informs our decisions. We do so by appealing to active inference, with a special focus on the oculomotor system. Within this exemplar system, we argue that the superior colliculus is well placed to act as a discrete-continuous interface. Interestingly, when the neuronal computations within the superior colliculus are formulated in terms of active inference, we find that many aspects of its neuroanatomy emerge from the computations it must perform in this role.},
  pmcid = {PMC6115199},
  pmid = {29894658},
  file = {/Users/phanthh/Zotero/storage/YQPHHJIM/Parr_Friston_2018_The Discrete and Continuous Brain.pdf}
}

@inproceedings{satorrasEquivariantGraphNeural2021,
  title = {E(n) {{Equivariant Graph Neural Networks}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Satorras, V{\'{\i}}ctor Garcia and Hoogeboom, Emiel and Welling, Max},
  year = {2021},
  month = jul,
  pages = {9323--9332},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {This paper introduces a new model to learn graph neural networks equivariant to rotations, translations, reflections and permutations called E(n)-Equivariant Graph Neural Networks (EGNNs). In contrast with existing methods, our work does not require computationally expensive higher-order representations in intermediate layers while it still achieves competitive or better performance. In addition, whereas existing methods are limited to equivariance on 3 dimensional spaces, our model is easily scaled to higher-dimensional spaces. We demonstrate the effectiveness of our method on dynamical systems modelling, representation learning in graph autoencoders and predicting molecular properties.},
  langid = {english},
  file = {/Users/phanthh/Zotero/storage/F4BEM6U5/Satorras et al_2021_E(n) Equivariant Graph Neural Networks.pdf;/Users/phanthh/Zotero/storage/HK8ES9XF/Satorras et al. - 2021 - E(n) Equivariant Graph Neural Networks.pdf}
}

@article{scarselliGraphNeuralNetwork2009,
  title = {The {{Graph Neural Network Model}}},
  author = {Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},
  year = {2009},
  month = jan,
  journal = {IEEE Transactions on Neural Networks},
  volume = {20},
  number = {1},
  pages = {61--80},
  issn = {1941-0093},
  doi = {10.1109/TNN.2008.2005605},
  abstract = {Many underlying relationships among data in several areas of science and engineering, e.g., computer vision, molecular chemistry, molecular biology, pattern recognition, and data mining, can be represented in terms of graphs. In this paper, we propose a new neural network model, called graph neural network (GNN) model, that extends existing neural network methods for processing the data represented in graph domains. This GNN model, which can directly process most of the practically useful types of graphs, e.g., acyclic, cyclic, directed, and undirected, implements a function tau(G,n) isin IRm that maps a graph G and one of its nodes n into an m-dimensional Euclidean space. A supervised learning algorithm is derived to estimate the parameters of the proposed GNN model. The computational cost of the proposed algorithm is also considered. Some experimental results are shown to validate the proposed learning algorithm, and to demonstrate its generalization capabilities.},
  keywords = {Biological system modeling,Biology,Chemistry,Computer vision,Data engineering,Data mining,graph neural networks (GNNs),graph processing,Graphical domains,Neural networks,Parameter estimation,Pattern recognition,recursive neural networks,Supervised learning},
  file = {/Users/phanthh/Zotero/storage/C36A3XA2/Scarselli et al. - 2009 - The Graph Neural Network Model.pdf;/Users/phanthh/Zotero/storage/JXBPKJNH/Scarselli et al_2009_The Graph Neural Network Model.pdf}
}

@inproceedings{schlichtkrullModelingRelationalData2018,
  title = {Modeling {{Relational Data}} with {{Graph Convolutional Networks}}},
  booktitle = {The {{Semantic Web}}},
  author = {Schlichtkrull, Michael and Kipf, Thomas N. and Bloem, Peter and {van~den Berg}, Rianne and Titov, Ivan and Welling, Max},
  editor = {Gangemi, Aldo and Navigli, Roberto and Vidal, Maria-Esther and Hitzler, Pascal and Troncy, Rapha{\"e}l and Hollink, Laura and Tordai, Anna and Alam, Mehwish},
  year = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {593--607},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-93417-4_38},
  abstract = {Knowledge graphs enable a wide variety of applications, including question answering and information retrieval. Despite the great effort invested in their creation and maintenance, even the largest (e.g., Yago, DBPedia or Wikidata) remain incomplete. We introduce Relational Graph Convolutional Networks (R-GCNs) and apply them to two standard knowledge base completion tasks: Link prediction (recovery of missing facts, i.e.~subject-predicate-object triples) and entity classification (recovery of missing entity attributes). R-GCNs are related to a recent class of neural networks operating on graphs, and are developed specifically to handle the highly multi-relational data characteristic of realistic knowledge bases. We demonstrate the effectiveness of R-GCNs as a stand-alone model for entity classification. We further show that factorization models for link prediction such as DistMult can be significantly improved through the use of an R-GCN encoder model to accumulate evidence over multiple inference steps in the graph, demonstrating a large improvement of 29.8\% on FB15k-237 over a decoder-only baseline.},
  isbn = {978-3-319-93417-4},
  langid = {english},
  file = {/Users/phanthh/Zotero/storage/AVAAH987/Schlichtkrull et al_2018_Modeling Relational Data with Graph Convolutional Networks.pdf;/Users/phanthh/Zotero/storage/VY7X2VK8/Schlichtkrull et al. - 2018 - Modeling Relational Data with Graph Convolutional .pdf}
}

@misc{shiGraphAFFlowbasedAutoregressive2020,
  title = {{{GraphAF}}: A {{Flow-based Autoregressive Model}} for {{Molecular Graph Generation}}},
  shorttitle = {{{GraphAF}}},
  author = {Shi, Chence and Xu, Minkai and Zhu, Zhaocheng and Zhang, Weinan and Zhang, Ming and Tang, Jian},
  year = {2020},
  month = feb,
  number = {arXiv:2001.09382},
  eprint = {2001.09382},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2001.09382},
  abstract = {Molecular graph generation is a fundamental problem for drug discovery and has been attracting growing attention. The problem is challenging since it requires not only generating chemically valid molecular structures but also optimizing their chemical properties in the meantime. Inspired by the recent progress in deep generative models, in this paper we propose a flow-based autoregressive model for graph generation called GraphAF. GraphAF combines the advantages of both autoregressive and flow-based approaches and enjoys: (1) high model flexibility for data density estimation; (2) efficient parallel computation for training; (3) an iterative sampling process, which allows leveraging chemical domain knowledge for valency checking. Experimental results show that GraphAF is able to generate 68\% chemically valid molecules even without chemical knowledge rules and 100\% valid molecules with chemical rules. The training process of GraphAF is two times faster than the existing state-of-the-art approach GCPN. After fine-tuning the model for goal-directed property optimization with reinforcement learning, GraphAF achieves state-of-the-art performance on both chemical property optimization and constrained property optimization.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/phanthh/Zotero/storage/VNAYY7VR/Shi et al_2020_GraphAF.pdf}
}

@inproceedings{simonovskyGraphVAEGenerationSmall2018,
  title = {{{GraphVAE}}: {{Towards Generation}} of {{Small Graphs Using Variational Autoencoders}}},
  shorttitle = {{{GraphVAE}}},
  booktitle = {Artificial {{Neural Networks}} and {{Machine Learning}} \textendash{} {{ICANN}} 2018},
  author = {Simonovsky, Martin and Komodakis, Nikos},
  editor = {K{\r{u}}rkov{\'a}, V{\v e}ra and Manolopoulos, Yannis and Hammer, Barbara and Iliadis, Lazaros and Maglogiannis, Ilias},
  year = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {412--422},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-01418-6_41},
  abstract = {Deep learning on graphs has become a popular research topic with many applications. However, past work has concentrated on learning graph embedding tasks, which is in contrast with advances in generative models for images and text. Is it possible to transfer this progress to the domain of graphs? We propose to sidestep hurdles associated with linearization of such discrete structures by having a decoder output a probabilistic fully-connected graph of a predefined maximum size directly at once. Our method is formulated as a variational autoencoder. We evaluate on the challenging task of molecule generation.},
  isbn = {978-3-030-01418-6},
  langid = {english},
  file = {/Users/phanthh/Zotero/storage/27G26MTF/Simonovsky_Komodakis_2018_GraphVAE.pdf;/Users/phanthh/Zotero/storage/AI5PVIML/Simonovsky and Komodakis - 2018 - GraphVAE Towards Generation of Small Graphs Using.pdf}
}

@misc{tomczakImprovingVariationalAutoEncoders2017,
  title = {Improving {{Variational Auto-Encoders}} Using Convex Combination Linear {{Inverse Autoregressive Flow}}},
  author = {Tomczak, Jakub M. and Welling, Max},
  year = {2017},
  month = jun,
  number = {arXiv:1706.02326},
  eprint = {1706.02326},
  eprinttype = {arxiv},
  primaryclass = {stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1706.02326},
  abstract = {In this paper, we propose a new volume-preserving flow and show that it performs similarly to the linear general normalizing flow. The idea is to enrich a linear Inverse Autoregressive Flow by introducing multiple lower-triangular matrices with ones on the diagonal and combining them using a convex combination. In the experimental studies on MNIST and Histopathology data we show that the proposed approach outperforms other volume-preserving flows and is competitive with current state-of-the-art linear normalizing flow.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Machine Learning},
  file = {/Users/phanthh/Zotero/storage/WFX4SSWC/Tomczak_Welling_2017_Improving Variational Auto-Encoders using convex combination linear Inverse.pdf;/Users/phanthh/Zotero/storage/YKBCW5II/1706.html}
}

@misc{velickovicGraphAttentionNetworks2018,
  title = {Graph {{Attention Networks}}},
  author = {Veli{\v c}kovi{\'c}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\`o}, Pietro and Bengio, Yoshua},
  year = {2018},
  month = feb,
  number = {arXiv:1710.10903},
  eprint = {1710.10903},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1710.10903},
  abstract = {We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Social and Information Networks,Statistics - Machine Learning},
  file = {/Users/phanthh/Zotero/storage/YPNFFJQ6/Veličković et al_2018_Graph Attention Networks.pdf}
}

@book{villaniTopicsOptimalTransportation2003,
  title = {Topics in {{Optimal Transportation}}},
  author = {Villani, C{\'e}dric},
  year = {2003},
  publisher = {{American Mathematical Soc.}},
  abstract = {Cedric Villani's book is a lucid and very readable documentation of the tremendous recent analytic progress in ``optimal mass transportation'' theory and of its diverse and unexpected applications in optimization, nonlinear PDE, geometry, and mathematical physics. --Lawrence C. Evans, University of California at Berkeley In 1781, Gaspard Monge defined the problem of ``optimal transportation'', or the transferring of mass with the least possible amount of work, with applications to engineering in mind. In 1942, Leonid Kantorovich applied the newborn machinery of linear programming to Monge's problem, with applications to economics in mind. In 1987, Yann Brenier used optimal transportation to prove a new projection theorem on the set of measure preserving maps, with applications to fluid mechanics in mind. Each of these contributions marked the beginning of a whole mathematical theory, with many unexpected ramifications. Nowadays, the Monge-Kantorovich problem is used and studied by researchers from extremely diverse horizons, including probability theory, functional analysis, isoperimetry, partial differential equations, and even meteorology. Originating from a graduate course, the present volume is at once an introduction to the field of optimal transportation and a survey of the research on the topic over the last 15 years. The book is intended for graduate students and researchers, and it covers both theory and applications. Readers are only assumed to be familiar with the basics of measure theory and functional analysis.},
  googlebooks = {idyFAwAAQBAJ},
  isbn = {978-0-8218-3312-4},
  langid = {english},
  keywords = {Mathematics / General}
}

@inproceedings{wehenkelUnconstrainedMonotonicNeural2019,
  title = {Unconstrained {{Monotonic Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Wehenkel, Antoine and Louppe, Gilles},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Monotonic neural networks have recently been proposed as a way to define invertible transformations. These transformations can be combined into powerful autoregressive flows that have been shown to be universal approximators of continuous probability distributions. Architectures that ensure monotonicity typically enforce constraints on weights and activation functions, which enables invertibility but leads to a cap on the expressiveness of the resulting transformations.  In this work, we propose the Unconstrained Monotonic Neural Network (UMNN) architecture based on the insight that a function is monotonic as long as its derivative is strictly positive. In particular, this latter condition can be enforced with a free-form neural network whose only constraint is the positiveness of its output.  We evaluate our new invertible building block within a new autoregressive flow (UMNN-MAF) and demonstrate its effectiveness on density estimation experiments.  We also illustrate the ability of UMNNs to improve variational inference.},
  file = {/Users/phanthh/Zotero/storage/4E7UWJ9P/Wehenkel_Louppe_2019_Unconstrained Monotonic Neural Networks.pdf}
}

@article{wuComprehensiveSurveyGraph2021,
  title = {A {{Comprehensive Survey}} on {{Graph Neural Networks}}},
  author = {Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Yu, Philip S.},
  year = {2021},
  month = jan,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {32},
  number = {1},
  eprint = {1901.00596},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  pages = {4--24},
  issn = {2162-237X, 2162-2388},
  doi = {10.1109/TNNLS.2020.2978386},
  abstract = {Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this survey, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art graph neural networks into four categories, namely recurrent graph neural networks, convolutional graph neural networks, graph autoencoders, and spatial-temporal graph neural networks. We further discuss the applications of graph neural networks across various domains and summarize the open source codes, benchmark data sets, and model evaluation of graph neural networks. Finally, we propose potential research directions in this rapidly growing field.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/phanthh/Zotero/storage/6QR695XB/Wu et al_2021_A Comprehensive Survey on Graph Neural Networks.pdf}
}

@article{xiaGraphbasedGenerativeModels2019,
  title = {Graph-Based Generative Models for de {{Novo}} Drug Design},
  author = {Xia, Xiaolin and Hu, Jianxing and Wang, Yanxing and Zhang, Liangren and Liu, Zhenming},
  year = {2019},
  month = dec,
  journal = {Drug Discovery Today: Technologies},
  series = {Artificial {{Intelligence}}},
  volume = {32--33},
  pages = {45--53},
  issn = {1740-6749},
  doi = {10.1016/j.ddtec.2020.11.004},
  abstract = {The discovery of new chemical entities is a crucial part of drug discovery, which requires the lead compounds to have desired properties to be pharmaceutically active. De novo drug design aims to generate and optimize novel ligands for macromolecular targets from scratch. The development of graph-based deep generative neural networks has provided a new method. In this review, we gave a brief introduction to graph representation and graph-based generative models for de novo drug design, summarized them as four architectures, and concluded each's characteristics. We also discussed generative models for scaffold- and fragment-based design and graph-based generative models' future directions.},
  langid = {english}
}

@inproceedings{youGraphRNNGeneratingRealistic2018,
  title = {{{GraphRNN}}: {{Generating Realistic Graphs}} with {{Deep Auto-regressive Models}}},
  shorttitle = {{{GraphRNN}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {You, Jiaxuan and Ying, Rex and Ren, Xiang and Hamilton, William and Leskovec, Jure},
  year = {2018},
  month = jul,
  pages = {5708--5717},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Modeling and generating graphs is fundamental for studying networks in biology, engineering, and social sciences. However, modeling complex distributions over graphs and then efficiently sampling from these distributions is challenging due to the non-unique, high-dimensional nature of graphs and the complex, non-local dependencies that exist between edges in a given graph. Here we propose GraphRNN, a deep autoregressive model that addresses the above challenges and approximates any distribution of graphs with minimal assumptions about their structure. GraphRNN learns to generate graphs by training on a representative set of graphs and decomposes the graph generation process into a sequence of node and edge formations, conditioned on the graph structure generated so far. In order to quantitatively evaluate the performance of GraphRNN, we introduce a benchmark suite of datasets, baselines and novel evaluation metrics based on Maximum Mean Discrepancy, which measure distances between sets of graphs. Our experiments show that GraphRNN significantly outperforms all baselines, learning to generate diverse graphs that match the structural characteristics of a target set, while also scaling to graphs 50 times larger than previous deep models.},
  langid = {english},
  file = {/Users/phanthh/Zotero/storage/8NSP8XRL/You et al. - 2018 - GraphRNN Generating Realistic Graphs with Deep Au.pdf;/Users/phanthh/Zotero/storage/GBFH9B3Z/You et al_2018_GraphRNN.pdf}
}

@inproceedings{zangMoFlowInvertibleFlow2020,
  title = {{{MoFlow}}: {{An Invertible Flow Model}} for {{Generating Molecular Graphs}}},
  shorttitle = {{{MoFlow}}},
  booktitle = {Proceedings of the 26th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Zang, Chengxi and Wang, Fei},
  year = {2020},
  month = aug,
  eprint = {2006.10137},
  eprinttype = {arxiv},
  primaryclass = {physics, stat},
  pages = {617--626},
  doi = {10.1145/3394486.3403104},
  abstract = {Generating molecular graphs with desired chemical properties driven by deep graph generative models provides a very promising way to accelerate drug discovery process. Such graph generative models usually consist of two steps: learning latent representations and generation of molecular graphs. However, to generate novel and chemically-valid molecular graphs from latent representations is very challenging because of the chemical constraints and combinatorial complexity of molecular graphs. In this paper, we propose MoFlow, a flow-based graph generative model to learn invertible mappings between molecular graphs and their latent representations. To generate molecular graphs, our MoFlow first generates bonds (edges) through a Glow based model, then generates atoms (nodes) given bonds by a novel graph conditional flow, and finally assembles them into a chemically valid molecular graph with a posthoc validity correction. Our MoFlow has merits including exact and tractable likelihood training, efficient one-pass embedding and generation, chemical validity guarantees, 100\textbackslash\% reconstruction of training data, and good generalization ability. We validate our model by four tasks: molecular graph generation and reconstruction, visualization of the continuous latent space, property optimization, and constrained property optimization. Our MoFlow achieves state-of-the-art performance, which implies its potential efficiency and effectiveness to explore large chemical space for drug discovery.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Physics - Chemical Physics,Statistics - Machine Learning},
  file = {/Users/phanthh/Zotero/storage/ZK2DFFHR/Zang_Wang_2020_MoFlow.pdf}
}

@misc{zhangApproximationCapabilitiesNeural2020,
  title = {Approximation {{Capabilities}} of {{Neural ODEs}} and {{Invertible Residual Networks}}},
  author = {Zhang, Han and Gao, Xi and Unterman, Jacob and Arodz, Tom},
  year = {2020},
  month = feb,
  number = {arXiv:1907.12998},
  eprint = {1907.12998},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1907.12998},
  abstract = {Neural ODEs and i-ResNet are recently proposed methods for enforcing invertibility of residual neural models. Having a generic technique for constructing invertible models can open new avenues for advances in learning systems, but so far the question of whether Neural ODEs and i-ResNets can model any continuous invertible function remained unresolved. Here, we show that both of these models are limited in their approximation capabilities. We then prove that any homeomorphism on a \$p\$-dimensional Euclidean space can be approximated by a Neural ODE operating on a \$2p\$-dimensional Euclidean space, and a similar result for i-ResNets. We conclude by showing that capping a Neural ODE or an i-ResNet with a single linear layer is sufficient to turn the model into a universal approximator for non-invertible continuous functions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/phanthh/Zotero/storage/H3WJ5ZR4/Zhang et al_2020_Approximation Capabilities of Neural ODEs and Invertible Residual Networks.pdf;/Users/phanthh/Zotero/storage/VN7RPPR9/1907.html}
}

@article{zhangDeepLearningGraphs2020,
  title = {Deep {{Learning}} on {{Graphs}}: {{A Survey}}},
  shorttitle = {Deep {{Learning}} on {{Graphs}}},
  author = {Zhang, Ziwei and Cui, Peng and Zhu, Wenwu},
  year = {2020},
  month = mar,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {34},
  number = {1},
  pages = {249--270},
  issn = {1558-2191},
  doi = {10.1109/TKDE.2020.2981333},
  abstract = {Deep learning has been shown to be successful in a number of domains, ranging from acoustics, images, to natural language processing. However, applying deep learning to the ubiquitous graph data is non-trivial because of the unique characteristics of graphs. Recently, substantial research efforts have been devoted to applying deep learning methods to graphs, resulting in beneficial advances in graph analysis techniques. In this survey, we comprehensively review the different types of deep learning methods on graphs. We divide the existing methods into five categories based on their model architectures and training strategies: graph recurrent neural networks, graph convolutional networks, graph autoencoders, graph reinforcement learning, and graph adversarial methods. We then provide a comprehensive overview of these methods in a systematic manner mainly by following their development history. We also analyze the differences and compositions of different methods. Finally, we briefly outline the applications in which they have been used and discuss potential future research directions.},
  keywords = {deep learning,Deep learning,Distance measurement,graph autoencoder,graph convolutional network,Graph data,graph neural network,Recurrent neural networks,Reinforcement learning,Social networking (online),Task analysis,Training},
  file = {/Users/phanthh/Zotero/storage/9F8GDS65/Zhang et al_2022_Deep Learning on Graphs.pdf;/Users/phanthh/Zotero/storage/X92JGH4W/Zhang et al. - 2022 - Deep Learning on Graphs A Survey.pdf}
}

@inproceedings{zhangDiffusionNormalizingFlow2021,
  title = {Diffusion {{Normalizing Flow}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zhang, Qinsheng and Chen, Yongxin},
  year = {2021},
  volume = {34},
  pages = {16280--16291},
  publisher = {{Curran Associates, Inc.}},
  abstract = {We present a novel generative modeling method called diffusion normalizing flow based on stochastic differential equations (SDEs). The algorithm consists of two neural SDEs: a forward SDE that gradually adds noise to the data to transform the data into Gaussian random noise, and a backward SDE that gradually removes the noise to sample from the data distribution. By jointly training the two neural SDEs to minimize a common cost function that quantifies the difference between the two, the backward SDE converges to a diffusion process the starts with a Gaussian distribution and ends with the desired data distribution. Our method is closely related to normalizing flow and diffusion probabilistic models, and can be viewed as a combination of the two. Compared with normalizing flow, diffusion normalizing flow is able to learn distributions with sharp boundaries. Compared with diffusion probabilistic models, diffusion normalizing flow requires fewer discretization steps and thus has better sampling efficiency. Our algorithm demonstrates competitive performance in both high-dimension data density estimation and image generation tasks.},
  file = {/Users/phanthh/Zotero/storage/KPV3L9ZS/Zhang_Chen_2021_Diffusion Normalizing Flow.pdf}
}

@article{zhouGraphNeuralNetworks2020,
  title = {Graph Neural Networks: {{A}} Review of Methods and Applications},
  shorttitle = {Graph Neural Networks},
  author = {Zhou, Jie and Cui, Ganqu and Hu, Shengding and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng and Sun, Maosong},
  year = {2020},
  month = jan,
  journal = {AI Open},
  volume = {1},
  pages = {57--81},
  issn = {2666-6510},
  doi = {10.1016/j.aiopen.2021.01.001},
  abstract = {Lots of learning tasks require dealing with graph data which contains rich relation information among elements. Modeling physics systems, learning molecular fingerprints, predicting protein interface, and classifying diseases demand a model to learn from graph inputs. In other domains such as learning from non-structural data like texts and images, reasoning on extracted structures (like the dependency trees of sentences and the scene graphs of images) is an important research topic which also needs graph reasoning models. Graph neural networks (GNNs) are neural models that capture the dependence of graphs via message passing between the nodes of graphs. In recent years, variants of GNNs such as graph convolutional network (GCN), graph attention network (GAT), graph recurrent network (GRN) have demonstrated ground-breaking performances on many deep learning tasks. In this survey, we propose a general design pipeline for GNN models and discuss the variants of each component, systematically categorize the applications, and propose four open problems for future research.},
  langid = {english},
  keywords = {Deep learning,Graph neural network},
  file = {/Users/phanthh/Zotero/storage/9D3SLJUQ/Zhou et al_2020_Graph neural networks.pdf}
}

@misc{zhuSurveyDeepGraph2022,
  title = {A {{Survey}} on {{Deep Graph Generation}}: {{Methods}} and {{Applications}}},
  shorttitle = {A {{Survey}} on {{Deep Graph Generation}}},
  author = {Zhu, Yanqiao and Du, Yuanqi and Wang, Yinkai and Xu, Yichen and Zhang, Jieyu and Liu, Qiang and Wu, Shu},
  year = {2022},
  month = mar,
  number = {arXiv:2203.06714},
  eprint = {2203.06714},
  eprinttype = {arxiv},
  primaryclass = {cs, q-bio},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2203.06714},
  abstract = {Graphs are ubiquitous in encoding relational information of real-world objects in many domains. Graph generation, whose purpose is to generate new graphs from a distribution similar to the observed graphs, has received increasing attention thanks to the recent advances of deep learning models. In this paper, we conduct a comprehensive review on the existing literature of graph generation from a variety of emerging methods to its wide application areas. Specifically, we first formulate the problem of deep graph generation and discuss its difference with several related graph learning tasks. Secondly, we divide the state-of-the-art methods into three categories based on model architectures and summarize their generation strategies. Thirdly, we introduce three key application areas of deep graph generation. Lastly, we highlight challenges and opportunities in the future study of deep graph generation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Social and Information Networks,Quantitative Biology - Molecular Networks},
  file = {/Users/phanthh/Zotero/storage/U2YX33JH/Zhu et al_2022_A Survey on Deep Graph Generation.pdf;/Users/phanthh/Zotero/storage/BTJ65JTZ/2203.html}
}

@inproceedings{zieglerLatentNormalizingFlows2019,
  title = {Latent {{Normalizing Flows}} for {{Discrete Sequences}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Ziegler, Zachary and Rush, Alexander},
  year = {2019},
  month = may,
  pages = {7673--7682},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Normalizing flows are a powerful class of generative models for continuous random variables, showing both strong model flexibility and the potential for non-autoregressive generation. These benefits are also desired when modeling discrete random variables such as text, but directly applying normalizing flows to discrete sequences poses significant additional challenges. We propose a VAE-based generative model which jointly learns a normalizing flow-based distribution in the latent space and a stochastic mapping to an observed discrete space. In this setting, we find that it is crucial for the flow-based distribution to be highly multimodal. To capture this property, we propose several normalizing flow architectures to maximize model flexibility. Experiments consider common discrete sequence tasks of character-level language modeling and polyphonic music generation. Our results indicate that an autoregressive flow-based model can match the performance of a comparable autoregressive baseline, and a non-autoregressive flow-based model can improve generation speed with a penalty to performance.},
  langid = {english},
  file = {/Users/phanthh/Zotero/storage/6Q2LEUV5/Ziegler and Rush - 2019 - Latent Normalizing Flows for Discrete Sequences.pdf;/Users/phanthh/Zotero/storage/VA6I9VFZ/Ziegler_Rush_2019_Latent Normalizing Flows for Discrete Sequences.pdf}
}


