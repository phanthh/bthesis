
@article{asifGraphNeuralNetwork2021,
  title = {Graph {{Neural Network}}: {{A Comprehensive Review}} on {{Non-Euclidean Space}}},
  shorttitle = {Graph {{Neural Network}}},
  author = {Asif, Nurul A. and Sarker, Yeahia and Chakrabortty, Ripon K. and Ryan, Michael J. and Ahamed, Md. Hafiz and Saha, Dip K. and Badal, Faisal R. and Das, Sajal K. and Ali, Md. Firoz and Moyeen, Sumaya I. and Islam, Md. Robiul and Tasneem, Zinat},
  year = {2021},
  journal = {IEEE Access},
  volume = {9},
  pages = {60588--60606},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3071274},
  abstract = {This review provides a comprehensive overview of the state-of-the-art methods of graph-based networks from a deep learning perspective. Graph networks provide a generalized form to exploit non-euclidean space data. A graph can be visualized as an aggregation of nodes and edges without having any order. Data-driven architecture tends to follow a fixed neural network trying to find the pattern in feature space. These strategies have successfully been applied to many applications for euclidean space data. Since graph data in a non-euclidean space does not follow any kind of order, these solutions can be applied to exploit the node relationships. Graph Neural Networks (GNNs) solve this problem by exploiting the relationships among graph data. Recent developments in computational hardware and optimization allow graph networks possible to learn the complex graph relationships. Graph networks are therefore being actively used to solve many problems including protein interface, classification, and learning representations of fingerprints. To encapsulate the importance of graph models, in this paper, we formulate a systematic categorization of GNN models according to their applications from theory to real-life problems and provide a direction of the future scope for the applications of graph models as well as highlight the limitations of existing graph networks.},
  keywords = {Computational modeling,Convolution,Feature extraction,geometric deep learning,Graph neural network,Graph neural networks,graph-structured network,Licenses,non-euclidean space,Task analysis,Taxonomy},
  file = {/home/phanthh/Academia/RP-Research Papers/storage/5AJKEFXP/Asif et al_2021_Graph Neural Network.pdf}
}

@misc{bojchevskiNetGANGeneratingGraphs2018,
  title = {{{NetGAN}}: {{Generating Graphs}} via {{Random Walks}}},
  shorttitle = {{{NetGAN}}},
  author = {Bojchevski, Aleksandar and Shchur, Oleksandr and Z{\"u}gner, Daniel and G{\"u}nnemann, Stephan},
  year = {2018},
  month = jun,
  number = {arXiv:1803.00816},
  eprint = {1803.00816},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1803.00816},
  url = {http://arxiv.org/abs/1803.00816},
  urldate = {2022-06-17},
  abstract = {We propose NetGAN - the first implicit generative model for graphs able to mimic real-world networks. We pose the problem of graph generation as learning the distribution of biased random walks over the input graph. The proposed model is based on a stochastic neural network that generates discrete output samples and is trained using the Wasserstein GAN objective. NetGAN is able to produce graphs that exhibit well-known network patterns without explicitly specifying them in the model definition. At the same time, our model exhibits strong generalization properties, as highlighted by its competitive link prediction performance, despite not being trained specifically for this task. Being the first approach to combine both of these desirable properties, NetGAN opens exciting avenues for further research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Social and Information Networks,Statistics - Machine Learning},
  file = {/home/phanthh/Academia/RP-Research Papers/storage/YWKWFX4V/Bojchevski et al_2018_NetGAN.pdf}
}

@article{bronsteinGeometricDeepLearning2017,
  title = {Geometric {{Deep Learning}}: {{Going}} beyond {{Euclidean}} Data},
  shorttitle = {Geometric {{Deep Learning}}},
  author = {Bronstein, Michael M. and Bruna, Joan and LeCun, Yann and Szlam, Arthur and Vandergheynst, Pierre},
  year = {2017},
  month = jul,
  journal = {IEEE Signal Processing Magazine},
  volume = {34},
  number = {4},
  pages = {18--42},
  issn = {1558-0792},
  doi = {10.1109/MSP.2017.2693418},
  abstract = {Geometric deep learning is an umbrella term for emerging techniques attempting to generalize (structured) deep neural models to non-Euclidean domains, such as graphs and manifolds. The purpose of this article is to overview different examples of geometric deep-learning problems and present available solutions, key difficulties, applications, and future research directions in this nascent field.},
  keywords = {Computational modeling,Computer architecture,Convolution,Convolutional codes,Euclidean distance,Machine learning,Social network services},
  file = {/home/phanthh/Academia/RP-Research Papers/storage/42CQKQ2H/Bronstein et al_2017_Geometric Deep Learning.pdf;/home/phanthh/Academia/RP-Research Papers/storage/CN9V7MS4/Bronstein et al. - 2017 - Geometric Deep Learning Going beyond Euclidean da.pdf}
}

@misc{chamiMachineLearningGraphs2022,
  title = {Machine {{Learning}} on {{Graphs}}: {{A Model}} and {{Comprehensive Taxonomy}}},
  shorttitle = {Machine {{Learning}} on {{Graphs}}},
  author = {Chami, Ines and {Abu-El-Haija}, Sami and Perozzi, Bryan and R{\'e}, Christopher and Murphy, Kevin},
  year = {2022},
  month = apr,
  number = {arXiv:2005.03675},
  eprint = {2005.03675},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2005.03675},
  url = {http://arxiv.org/abs/2005.03675},
  urldate = {2022-06-20},
  abstract = {There has been a surge of recent interest in learning representations for graph-structured data. Graph representation learning methods have generally fallen into three main categories, based on the availability of labeled data. The first, network embedding (such as shallow graph embedding or graph auto-encoders), focuses on learning unsupervised representations of relational structure. The second, graph regularized neural networks, leverages graphs to augment neural network losses with a regularization objective for semi-supervised learning. The third, graph neural networks, aims to learn differentiable functions over discrete topologies with arbitrary structure. However, despite the popularity of these areas there has been surprisingly little work on unifying the three paradigms. Here, we aim to bridge the gap between graph neural networks, network embedding and graph regularization models. We propose a comprehensive taxonomy of representation learning methods for graph-structured data, aiming to unify several disparate bodies of work. Specifically, we propose a Graph Encoder Decoder Model (GRAPHEDM), which generalizes popular algorithms for semi-supervised learning on graphs (e.g. GraphSage, Graph Convolutional Networks, Graph Attention Networks), and unsupervised learning of graph representations (e.g. DeepWalk, node2vec, etc) into a single consistent approach. To illustrate the generality of this approach, we fit over thirty existing methods into this framework. We believe that this unifying view both provides a solid foundation for understanding the intuition behind these methods, and enables future research in the area.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Computer Science - Social and Information Networks,Statistics - Machine Learning},
  file = {/home/phanthh/Academia/RP-Research Papers/storage/VMHJ3237/Chami et al_2022_Machine Learning on Graphs.pdf}
}

@inproceedings{daiSyntaxdirectedVariationalAutoencoder2018,
  title = {Syntax-Directed Variational Autoencoder for Molecule Generation},
  booktitle = {Proceedings of the International Conference on Learning Representations},
  author = {Dai, Hanjun and Tian, Yingtao and Dai, Bo and Skiena, Steven and Song, Le},
  year = {2018},
  file = {/home/phanthh/Academia/RP-Research Papers/storage/2IGRCAIV/Dai et al_2018_Syntax-directed variational autoencoder for molecule generation.pdf}
}

@misc{decaoMolGANImplicitGenerative2018,
  title = {{{MolGAN}}: {{An}} Implicit Generative Model for Small Molecular Graphs},
  shorttitle = {{{MolGAN}}},
  author = {De Cao, Nicola and Kipf, Thomas},
  year = {2018},
  month = may,
  number = {arXiv:1805.11973},
  eprint = {1805.11973},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1805.11973},
  url = {http://arxiv.org/abs/1805.11973},
  urldate = {2022-06-17},
  abstract = {Deep generative models for graph-structured data offer a new angle on the problem of chemical synthesis: by optimizing differentiable models that directly generate molecular graphs, it is possible to side-step expensive search procedures in the discrete and vast space of chemical structures. We introduce MolGAN, an implicit, likelihood-free generative model for small molecular graphs that circumvents the need for expensive graph matching procedures or node ordering heuristics of previous likelihood-based methods. Our method adapts generative adversarial networks (GANs) to operate directly on graph-structured data. We combine our approach with a reinforcement learning objective to encourage the generation of molecules with specific desired chemical properties. In experiments on the QM9 chemical database, we demonstrate that our model is capable of generating close to 100\% valid compounds. MolGAN compares favorably both to recent proposals that use string-based (SMILES) representations of molecules and to a likelihood-based method that directly generates graphs, albeit being susceptible to mode collapse.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/phanthh/Academia/RP-Research Papers/storage/582JGRZW/De Cao_Kipf_2018_MolGAN.pdf}
}

@misc{decaoMolGANImplicitGenerative2018a,
  title = {{{MolGAN}}: {{An}} Implicit Generative Model for Small Molecular Graphs},
  shorttitle = {{{MolGAN}}},
  author = {De Cao, Nicola and Kipf, Thomas},
  year = {2018},
  month = may,
  number = {arXiv:1805.11973},
  eprint = {1805.11973},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1805.11973},
  url = {http://arxiv.org/abs/1805.11973},
  urldate = {2022-06-20},
  abstract = {Deep generative models for graph-structured data offer a new angle on the problem of chemical synthesis: by optimizing differentiable models that directly generate molecular graphs, it is possible to side-step expensive search procedures in the discrete and vast space of chemical structures. We introduce MolGAN, an implicit, likelihood-free generative model for small molecular graphs that circumvents the need for expensive graph matching procedures or node ordering heuristics of previous likelihood-based methods. Our method adapts generative adversarial networks (GANs) to operate directly on graph-structured data. We combine our approach with a reinforcement learning objective to encourage the generation of molecules with specific desired chemical properties. In experiments on the QM9 chemical database, we demonstrate that our model is capable of generating close to 100\% valid compounds. MolGAN compares favorably both to recent proposals that use string-based (SMILES) representations of molecules and to a likelihood-based method that directly generates graphs, albeit being susceptible to mode collapse.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/phanthh/Academia/RP-Research Papers/storage/KC3ZPZUP/De Cao_Kipf_2018_MolGAN.pdf}
}

@article{gomez-bombarelliAutomaticChemicalDesign2018,
  title = {Automatic {{Chemical Design Using}} a {{Data-Driven Continuous Representation}} of {{Molecules}}},
  author = {{G{\'o}mez-Bombarelli}, Rafael and Wei, Jennifer N. and Duvenaud, David and {Hern{\'a}ndez-Lobato}, Jos{\'e} Miguel and {S{\'a}nchez-Lengeling}, Benjam{\'i}n and Sheberla, Dennis and {Aguilera-Iparraguirre}, Jorge and Hirzel, Timothy D. and Adams, Ryan P. and {Aspuru-Guzik}, Al{\'a}n},
  year = {2018},
  month = feb,
  journal = {ACS Central Science},
  volume = {4},
  number = {2},
  pages = {268--276},
  publisher = {{American Chemical Society}},
  issn = {2374-7943},
  doi = {10.1021/acscentsci.7b00572},
  url = {https://doi.org/10.1021/acscentsci.7b00572},
  urldate = {2022-06-17},
  abstract = {We report a method to convert discrete representations of molecules to and from a multidimensional continuous representation. This model allows us to generate new molecules for efficient exploration and optimization through open-ended spaces of chemical compounds. A deep neural network was trained on hundreds of thousands of existing chemical structures to construct three coupled functions: an encoder, a decoder, and a predictor. The encoder converts the discrete representation of a molecule into a real-valued continuous vector, and the decoder converts these continuous vectors back to discrete molecular representations. The predictor estimates chemical properties from the latent continuous vector representation of the molecule. Continuous representations of molecules allow us to automatically generate novel chemical structures by performing simple operations in the latent space, such as decoding random vectors, perturbing known chemical structures, or interpolating between molecules. Continuous representations also allow the use of powerful gradient-based optimization to efficiently guide the search for optimized functional compounds. We demonstrate our method in the domain of drug-like molecules and also in a set of molecules with fewer that nine heavy atoms.},
  file = {/home/phanthh/Academia/RP-Research Papers/storage/3CSBRCZS/Gómez-Bombarelli et al. - 2018 - Automatic Chemical Design Using a Data-Driven Cont.pdf;/home/phanthh/Academia/RP-Research Papers/storage/RI2V9QCI/Gómez-Bombarelli et al_2018_Automatic Chemical Design Using a Data-Driven Continuous Representation of.pdf}
}

@misc{grathwohlFFJORDFreeformContinuous2018,
  title = {{{FFJORD}}: {{Free-form Continuous Dynamics}} for {{Scalable Reversible Generative Models}}},
  shorttitle = {{{FFJORD}}},
  author = {Grathwohl, Will and Chen, Ricky T. Q. and Bettencourt, Jesse and Sutskever, Ilya and Duvenaud, David},
  year = {2018},
  month = oct,
  number = {arXiv:1810.01367},
  eprint = {1810.01367},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1810.01367},
  url = {http://arxiv.org/abs/1810.01367},
  urldate = {2022-06-10},
  abstract = {A promising class of generative models maps points from a simple distribution to a complex distribution through an invertible neural network. Likelihood-based training of these models requires restricting their architectures to allow cheap computation of Jacobian determinants. Alternatively, the Jacobian trace can be used if the transformation is specified by an ordinary differential equation. In this paper, we use Hutchinson's trace estimator to give a scalable unbiased estimate of the log-density. The result is a continuous-time invertible generative model with unbiased density estimation and one-pass sampling, while allowing unrestricted neural network architectures. We demonstrate our approach on high-dimensional density estimation, image generation, and variational inference, achieving the state-of-the-art among exact likelihood methods with efficient sampling.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/phanthh/Academia/RP-Research Papers/storage/AAAPHXZZ/Grathwohl et al_2018_FFJORD.pdf}
}

@inproceedings{hersheyCNNArchitecturesLargescale2017,
  title = {{{CNN}} Architectures for Large-Scale Audio Classification},
  booktitle = {2017 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Hershey, Shawn and Chaudhuri, Sourish and Ellis, Daniel P. W. and Gemmeke, Jort F. and Jansen, Aren and Moore, R. Channing and Plakal, Manoj and Platt, Devin and Saurous, Rif A. and Seybold, Bryan and Slaney, Malcolm and Weiss, Ron J. and Wilson, Kevin},
  year = {2017},
  month = mar,
  pages = {131--135},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2017.7952132},
  abstract = {Convolutional Neural Networks (CNNs) have proven very effective in image classification and show promise for audio. We use various CNN architectures to classify the soundtracks of a dataset of 70M training videos (5.24 million hours) with 30,871 video-level labels. We examine fully connected Deep Neural Networks (DNNs), AlexNet [1], VGG [2], Inception [3], and ResNet [4]. We investigate varying the size of both training set and label vocabulary, finding that analogs of the CNNs used in image classification do well on our audio classification task, and larger training and label sets help up to a point. A model using embeddings from these classifiers does much better than raw features on the Audio Set [5] Acoustic Event Detection (AED) classification task.},
  keywords = {Acoustic Event Detection,Acoustic Scene Classification,Computer architecture,Convolutional Neural Networks,Deep Neural Networks,Hidden Markov models,Neural networks,Servers,Spectrogram,Training,Video Classification,Videos},
  file = {/home/phanthh/Academia/RP-Research Papers/storage/BUB78VMJ/Hershey et al. - 2017 - CNN architectures for large-scale audio classifica.pdf;/home/phanthh/Academia/RP-Research Papers/storage/YU657UH2/Hershey et al_2017_CNN architectures for large-scale audio classification.pdf}
}

@misc{hondaGraphResidualFlow2019,
  title = {Graph {{Residual Flow}} for {{Molecular Graph Generation}}},
  author = {Honda, Shion and Akita, Hirotaka and Ishiguro, Katsuhiko and Nakanishi, Toshiki and Oono, Kenta},
  year = {2019},
  month = sep,
  number = {arXiv:1909.13521},
  eprint = {1909.13521},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1909.13521},
  url = {http://arxiv.org/abs/1909.13521},
  urldate = {2022-06-19},
  abstract = {Statistical generative models for molecular graphs attract attention from many researchers from the fields of bio- and chemo-informatics. Among these models, invertible flow-based approaches are not fully explored yet. In this paper, we propose a powerful invertible flow for molecular graphs, called graph residual flow (GRF). The GRF is based on residual flows, which are known for more flexible and complex non-linear mappings than traditional coupling flows. We theoretically derive non-trivial conditions such that GRF is invertible, and present a way of keeping the entire flows invertible throughout the training and sampling. Experimental results show that a generative model based on the proposed GRF achieves comparable generation performance, with much smaller number of trainable parameters compared to the existing flow-based model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/phanthh/Academia/RP-Research Papers/storage/B4JLWP6M/Honda et al_2019_Graph Residual Flow for Molecular Graph Generation.pdf}
}

@misc{kingmaAutoEncodingVariationalBayes2014,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2014},
  month = may,
  number = {arXiv:1312.6114},
  eprint = {1312.6114},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1312.6114},
  url = {http://arxiv.org/abs/1312.6114},
  urldate = {2022-06-17},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/phanthh/Academia/RP-Research Papers/storage/WAV6AWW7/Kingma_Welling_2014_Auto-Encoding Variational Bayes.pdf}
}

@inproceedings{kingmaGlowGenerativeFlow2018,
  title = {Glow: {{Generative Flow}} with {{Invertible}} 1x1 {{Convolutions}}},
  shorttitle = {Glow},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Kingma, Durk P and Dhariwal, Prafulla},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2018/hash/d139db6a236200b21cc7f752979132d0-Abstract.html},
  urldate = {2022-06-19},
  abstract = {Flow-based generative models are conceptually attractive due to tractability of the exact log-likelihood, tractability of exact latent-variable inference, and parallelizability of both training and synthesis. In this paper we propose Glow, a simple type of generative flow using invertible 1x1 convolution. Using our method we demonstrate a significant improvement in log-likelihood and qualitative sample quality. Perhaps most strikingly, we demonstrate that a generative model optimized towards the plain log-likelihood objective is capable of efficient synthesis of large and subjectively realistic-looking images.},
  file = {/home/phanthh/Academia/RP-Research Papers/storage/HYBBMZT2/Kingma_Dhariwal_2018_Glow.pdf}
}

@misc{kipfSemiSupervisedClassificationGraph2017,
  title = {Semi-{{Supervised Classification}} with {{Graph Convolutional Networks}}},
  author = {Kipf, Thomas N. and Welling, Max},
  year = {2017},
  month = feb,
  number = {arXiv:1609.02907},
  eprint = {1609.02907},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1609.02907},
  url = {http://arxiv.org/abs/1609.02907},
  urldate = {2022-06-17},
  abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/phanthh/Academia/RP-Research Papers/storage/EQILANNC/Kipf_Welling_2017_Semi-Supervised Classification with Graph Convolutional Networks.pdf}
}

@article{kobyzevNormalizingFlowsIntroduction2021,
  title = {Normalizing {{Flows}}: {{An Introduction}} and {{Review}} of {{Current Methods}}},
  shorttitle = {Normalizing {{Flows}}},
  author = {Kobyzev, Ivan and Prince, Simon J. D. and Brubaker, Marcus A.},
  year = {2021},
  month = nov,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {43},
  number = {11},
  eprint = {1908.09257},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  pages = {3964--3979},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2020.2992934},
  url = {http://arxiv.org/abs/1908.09257},
  urldate = {2022-06-10},
  abstract = {Normalizing Flows are generative models which produce tractable distributions where both sampling and density evaluation can be efficient and exact. The goal of this survey article is to give a coherent and comprehensive review of the literature around the construction and use of Normalizing Flows for distribution learning. We aim to provide context and explanation of the models, review current state-of-the-art literature, and identify open questions and promising future directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/phanthh/Academia/RP-Research Papers/storage/KTY3C8ZS/Kobyzev et al. - 2021 - Normalizing Flows An Introduction and Review of C.pdf}
}

@inproceedings{kusnerGrammarVariationalAutoencoder2017,
  title = {Grammar {{Variational Autoencoder}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Kusner, Matt J. and Paige, Brooks and {Hern{\'a}ndez-Lobato}, Jos{\'e} Miguel},
  year = {2017},
  month = jul,
  pages = {1945--1954},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v70/kusner17a.html},
  urldate = {2022-06-17},
  abstract = {Deep generative models have been wildly successful at learning coherent latent representations for continuous data such as natural images, artwork, and audio. However, generative modeling of discrete data such as arithmetic expressions and molecular structures still poses significant challenges. Crucially, state-of-the-art methods often produce outputs that are not valid. We make the key observation that frequently, discrete data can be represented as a parse tree from a context-free grammar. We propose a variational autoencoder which directly encodes from and decodes to these parse trees, ensuring the generated outputs are always syntactically valid. Surprisingly, we show that not only does our model more often generate valid outputs, it also learns a more coherent latent space in which nearby points decode to similar discrete outputs. We demonstrate the effectiveness of our learned models by showing their improved performance in Bayesian optimization for symbolic regression and molecule generation.},
  langid = {english},
  file = {/home/phanthh/Academia/RP-Research Papers/storage/6LN5RCJJ/Kusner et al_2017_Grammar Variational Autoencoder.pdf;/home/phanthh/Academia/RP-Research Papers/storage/V6ZDJQWH/Kusner et al. - 2017 - Grammar Variational Autoencoder.pdf}
}

@misc{liLearningDeepGenerative2018,
  title = {Learning {{Deep Generative Models}} of {{Graphs}}},
  author = {Li, Yujia and Vinyals, Oriol and Dyer, Chris and Pascanu, Razvan and Battaglia, Peter},
  year = {2018},
  month = mar,
  number = {arXiv:1803.03324},
  eprint = {1803.03324},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1803.03324},
  url = {http://arxiv.org/abs/1803.03324},
  urldate = {2022-06-17},
  abstract = {Graphs are fundamental data structures which concisely capture the relational structure in many important real-world domains, such as knowledge graphs, physical and social interactions, language, and chemistry. Here we introduce a powerful new approach for learning generative models over graphs, which can capture both their structure and attributes. Our approach uses graph neural networks to express probabilistic dependencies among a graph's nodes and edges, and can, in principle, learn distributions over any arbitrary graph. In a series of experiments our results show that once trained, our models can generate good quality samples of both synthetic graphs as well as real molecular graphs, both unconditionally and conditioned on data. Compared to baselines that do not use graph-structured representations, our models often perform far better. We also explore key challenges of learning generative models of graphs, such as how to handle symmetries and ordering of elements during the graph generation process, and offer possible solutions. Our work is the first and most general approach for learning generative models over arbitrary graphs, and opens new directions for moving away from restrictions of vector- and sequence-like knowledge representations, toward more expressive and flexible relational data structures.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/phanthh/Academia/RP-Research Papers/storage/RK5PXJQZ/Li et al_2018_Learning Deep Generative Models of Graphs.pdf}
}

@misc{madhawaGraphNVPInvertibleFlow2019,
  title = {{{GraphNVP}}: {{An Invertible Flow Model}} for {{Generating Molecular Graphs}}},
  shorttitle = {{{GraphNVP}}},
  author = {Madhawa, Kaushalya and Ishiguro, Katushiko and Nakago, Kosuke and Abe, Motoki},
  year = {2019},
  month = may,
  number = {arXiv:1905.11600},
  eprint = {1905.11600},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1905.11600},
  url = {http://arxiv.org/abs/1905.11600},
  urldate = {2022-06-10},
  abstract = {We propose GraphNVP, the first invertible, normalizing flow-based molecular graph generation model. We decompose the generation of a graph into two steps: generation of (i) an adjacency tensor and (ii) node attributes. This decomposition yields the exact likelihood maximization on graph-structured data, combined with two novel reversible flows. We empirically demonstrate that our model efficiently generates valid molecular graphs with almost no duplicated molecules. In addition, we observe that the learned latent space can be used to generate molecules with desired chemical properties.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/phanthh/Academia/RP-Research Papers/storage/QXVDNVRE/Madhawa et al_2019_GraphNVP.pdf}
}

@inproceedings{satorrasEquivariantGraphNeural2021,
  title = {E(n) {{Equivariant Graph Neural Networks}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Satorras, V{\'{\i}}ctor Garcia and Hoogeboom, Emiel and Welling, Max},
  year = {2021},
  month = jul,
  pages = {9323--9332},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v139/satorras21a.html},
  urldate = {2022-06-19},
  abstract = {This paper introduces a new model to learn graph neural networks equivariant to rotations, translations, reflections and permutations called E(n)-Equivariant Graph Neural Networks (EGNNs). In contrast with existing methods, our work does not require computationally expensive higher-order representations in intermediate layers while it still achieves competitive or better performance. In addition, whereas existing methods are limited to equivariance on 3 dimensional spaces, our model is easily scaled to higher-dimensional spaces. We demonstrate the effectiveness of our method on dynamical systems modelling, representation learning in graph autoencoders and predicting molecular properties.},
  langid = {english},
  file = {/home/phanthh/Academia/RP-Research Papers/storage/GJ4JZUDA/Satorras et al_2021_E(n) Equivariant Graph Neural Networks.pdf;/home/phanthh/Academia/RP-Research Papers/storage/UHXIYECM/Satorras et al. - 2021 - E(n) Equivariant Graph Neural Networks.pdf}
}

@article{scarselliGraphNeuralNetwork2009,
  title = {The {{Graph Neural Network Model}}},
  author = {Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},
  year = {2009},
  month = jan,
  journal = {IEEE Transactions on Neural Networks},
  volume = {20},
  number = {1},
  pages = {61--80},
  issn = {1941-0093},
  doi = {10.1109/TNN.2008.2005605},
  abstract = {Many underlying relationships among data in several areas of science and engineering, e.g., computer vision, molecular chemistry, molecular biology, pattern recognition, and data mining, can be represented in terms of graphs. In this paper, we propose a new neural network model, called graph neural network (GNN) model, that extends existing neural network methods for processing the data represented in graph domains. This GNN model, which can directly process most of the practically useful types of graphs, e.g., acyclic, cyclic, directed, and undirected, implements a function tau(G,n) isin IRm that maps a graph G and one of its nodes n into an m-dimensional Euclidean space. A supervised learning algorithm is derived to estimate the parameters of the proposed GNN model. The computational cost of the proposed algorithm is also considered. Some experimental results are shown to validate the proposed learning algorithm, and to demonstrate its generalization capabilities.},
  keywords = {Biological system modeling,Biology,Chemistry,Computer vision,Data engineering,Data mining,graph neural networks (GNNs),graph processing,Graphical domains,Neural networks,Parameter estimation,Pattern recognition,recursive neural networks,Supervised learning},
  file = {/home/phanthh/Academia/RP-Research Papers/storage/BKX2M3QG/Scarselli et al_2009_The Graph Neural Network Model.pdf;/home/phanthh/Academia/RP-Research Papers/storage/DJKN3KJD/Scarselli et al. - 2009 - The Graph Neural Network Model.pdf}
}

@inproceedings{schlichtkrullModelingRelationalData2018,
  title = {Modeling {{Relational Data}} with {{Graph Convolutional Networks}}},
  booktitle = {The {{Semantic Web}}},
  author = {Schlichtkrull, Michael and Kipf, Thomas N. and Bloem, Peter and {van~den Berg}, Rianne and Titov, Ivan and Welling, Max},
  editor = {Gangemi, Aldo and Navigli, Roberto and Vidal, Maria-Esther and Hitzler, Pascal and Troncy, Rapha{\"e}l and Hollink, Laura and Tordai, Anna and Alam, Mehwish},
  year = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {593--607},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-93417-4_38},
  abstract = {Knowledge graphs enable a wide variety of applications, including question answering and information retrieval. Despite the great effort invested in their creation and maintenance, even the largest (e.g., Yago, DBPedia or Wikidata) remain incomplete. We introduce Relational Graph Convolutional Networks (R-GCNs) and apply them to two standard knowledge base completion tasks: Link prediction (recovery of missing facts, i.e.~subject-predicate-object triples) and entity classification (recovery of missing entity attributes). R-GCNs are related to a recent class of neural networks operating on graphs, and are developed specifically to handle the highly multi-relational data characteristic of realistic knowledge bases. We demonstrate the effectiveness of R-GCNs as a stand-alone model for entity classification. We further show that factorization models for link prediction such as DistMult can be significantly improved through the use of an R-GCN encoder model to accumulate evidence over multiple inference steps in the graph, demonstrating a large improvement of 29.8\% on FB15k-237 over a decoder-only baseline.},
  isbn = {978-3-319-93417-4},
  langid = {english},
  file = {/home/phanthh/Academia/RP-Research Papers/storage/A93ILKE6/Schlichtkrull et al. - 2018 - Modeling Relational Data with Graph Convolutional .pdf;/home/phanthh/Academia/RP-Research Papers/storage/UB3NP9NK/Schlichtkrull et al_2018_Modeling Relational Data with Graph Convolutional Networks.pdf}
}

@misc{shiGraphAFFlowbasedAutoregressive2020,
  title = {{{GraphAF}}: A {{Flow-based Autoregressive Model}} for {{Molecular Graph Generation}}},
  shorttitle = {{{GraphAF}}},
  author = {Shi, Chence and Xu, Minkai and Zhu, Zhaocheng and Zhang, Weinan and Zhang, Ming and Tang, Jian},
  year = {2020},
  month = feb,
  number = {arXiv:2001.09382},
  eprint = {2001.09382},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2001.09382},
  url = {http://arxiv.org/abs/2001.09382},
  urldate = {2022-06-10},
  abstract = {Molecular graph generation is a fundamental problem for drug discovery and has been attracting growing attention. The problem is challenging since it requires not only generating chemically valid molecular structures but also optimizing their chemical properties in the meantime. Inspired by the recent progress in deep generative models, in this paper we propose a flow-based autoregressive model for graph generation called GraphAF. GraphAF combines the advantages of both autoregressive and flow-based approaches and enjoys: (1) high model flexibility for data density estimation; (2) efficient parallel computation for training; (3) an iterative sampling process, which allows leveraging chemical domain knowledge for valency checking. Experimental results show that GraphAF is able to generate 68\% chemically valid molecules even without chemical knowledge rules and 100\% valid molecules with chemical rules. The training process of GraphAF is two times faster than the existing state-of-the-art approach GCPN. After fine-tuning the model for goal-directed property optimization with reinforcement learning, GraphAF achieves state-of-the-art performance on both chemical property optimization and constrained property optimization.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/phanthh/Academia/RP-Research Papers/storage/XY3Y6EU5/Shi et al_2020_GraphAF.pdf}
}

@inproceedings{simonovskyGraphVAEGenerationSmall2018,
  title = {{{GraphVAE}}: {{Towards Generation}} of {{Small Graphs Using Variational Autoencoders}}},
  shorttitle = {{{GraphVAE}}},
  booktitle = {Artificial {{Neural Networks}} and {{Machine Learning}} \textendash{} {{ICANN}} 2018},
  author = {Simonovsky, Martin and Komodakis, Nikos},
  editor = {K{\r{u}}rkov{\'a}, V{\v e}ra and Manolopoulos, Yannis and Hammer, Barbara and Iliadis, Lazaros and Maglogiannis, Ilias},
  year = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {412--422},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-01418-6_41},
  abstract = {Deep learning on graphs has become a popular research topic with many applications. However, past work has concentrated on learning graph embedding tasks, which is in contrast with advances in generative models for images and text. Is it possible to transfer this progress to the domain of graphs? We propose to sidestep hurdles associated with linearization of such discrete structures by having a decoder output a probabilistic fully-connected graph of a predefined maximum size directly at once. Our method is formulated as a variational autoencoder. We evaluate on the challenging task of molecule generation.},
  isbn = {978-3-030-01418-6},
  langid = {english},
  file = {/home/phanthh/Academia/RP-Research Papers/storage/4CP5S468/Simonovsky and Komodakis - 2018 - GraphVAE Towards Generation of Small Graphs Using.pdf;/home/phanthh/Academia/RP-Research Papers/storage/5EXVU27Q/Simonovsky_Komodakis_2018_GraphVAE.pdf}
}

@misc{velickovicGraphAttentionNetworks2018,
  title = {Graph {{Attention Networks}}},
  author = {Veli{\v c}kovi{\'c}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Li{\`o}, Pietro and Bengio, Yoshua},
  year = {2018},
  month = feb,
  number = {arXiv:1710.10903},
  eprint = {1710.10903},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1710.10903},
  url = {http://arxiv.org/abs/1710.10903},
  urldate = {2022-06-17},
  abstract = {We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Social and Information Networks,Statistics - Machine Learning},
  file = {/home/phanthh/Academia/RP-Research Papers/storage/KL4W6WID/Veličković et al_2018_Graph Attention Networks.pdf}
}

@article{wuComprehensiveSurveyGraph2021,
  title = {A {{Comprehensive Survey}} on {{Graph Neural Networks}}},
  author = {Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Yu, Philip S.},
  year = {2021},
  month = jan,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {32},
  number = {1},
  eprint = {1901.00596},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  pages = {4--24},
  issn = {2162-237X, 2162-2388},
  doi = {10.1109/TNNLS.2020.2978386},
  url = {http://arxiv.org/abs/1901.00596},
  urldate = {2022-06-10},
  abstract = {Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this survey, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art graph neural networks into four categories, namely recurrent graph neural networks, convolutional graph neural networks, graph autoencoders, and spatial-temporal graph neural networks. We further discuss the applications of graph neural networks across various domains and summarize the open source codes, benchmark data sets, and model evaluation of graph neural networks. Finally, we propose potential research directions in this rapidly growing field.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/phanthh/Academia/RP-Research Papers/storage/WUNQI6Y2/Wu et al. - 2021 - A Comprehensive Survey on Graph Neural Networks.pdf}
}

@inproceedings{youGraphRNNGeneratingRealistic2018,
  title = {{{GraphRNN}}: {{Generating Realistic Graphs}} with {{Deep Auto-regressive Models}}},
  shorttitle = {{{GraphRNN}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {You, Jiaxuan and Ying, Rex and Ren, Xiang and Hamilton, William and Leskovec, Jure},
  year = {2018},
  month = jul,
  pages = {5708--5717},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v80/you18a.html},
  urldate = {2022-06-17},
  abstract = {Modeling and generating graphs is fundamental for studying networks in biology, engineering, and social sciences. However, modeling complex distributions over graphs and then efficiently sampling from these distributions is challenging due to the non-unique, high-dimensional nature of graphs and the complex, non-local dependencies that exist between edges in a given graph. Here we propose GraphRNN, a deep autoregressive model that addresses the above challenges and approximates any distribution of graphs with minimal assumptions about their structure. GraphRNN learns to generate graphs by training on a representative set of graphs and decomposes the graph generation process into a sequence of node and edge formations, conditioned on the graph structure generated so far. In order to quantitatively evaluate the performance of GraphRNN, we introduce a benchmark suite of datasets, baselines and novel evaluation metrics based on Maximum Mean Discrepancy, which measure distances between sets of graphs. Our experiments show that GraphRNN significantly outperforms all baselines, learning to generate diverse graphs that match the structural characteristics of a target set, while also scaling to graphs 50 times larger than previous deep models.},
  langid = {english},
  file = {/home/phanthh/Academia/RP-Research Papers/storage/B5WDRGV6/You et al. - 2018 - GraphRNN Generating Realistic Graphs with Deep Au.pdf;/home/phanthh/Academia/RP-Research Papers/storage/CBVQZMZ4/You et al_2018_GraphRNN.pdf}
}

@inproceedings{zangMoFlowInvertibleFlow2020,
  title = {{{MoFlow}}: {{An Invertible Flow Model}} for {{Generating Molecular Graphs}}},
  shorttitle = {{{MoFlow}}},
  booktitle = {Proceedings of the 26th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Zang, Chengxi and Wang, Fei},
  year = {2020},
  month = aug,
  eprint = {2006.10137},
  eprinttype = {arxiv},
  primaryclass = {physics, stat},
  pages = {617--626},
  doi = {10.1145/3394486.3403104},
  url = {http://arxiv.org/abs/2006.10137},
  urldate = {2022-06-10},
  abstract = {Generating molecular graphs with desired chemical properties driven by deep graph generative models provides a very promising way to accelerate drug discovery process. Such graph generative models usually consist of two steps: learning latent representations and generation of molecular graphs. However, to generate novel and chemically-valid molecular graphs from latent representations is very challenging because of the chemical constraints and combinatorial complexity of molecular graphs. In this paper, we propose MoFlow, a flow-based graph generative model to learn invertible mappings between molecular graphs and their latent representations. To generate molecular graphs, our MoFlow first generates bonds (edges) through a Glow based model, then generates atoms (nodes) given bonds by a novel graph conditional flow, and finally assembles them into a chemically valid molecular graph with a posthoc validity correction. Our MoFlow has merits including exact and tractable likelihood training, efficient one-pass embedding and generation, chemical validity guarantees, 100\textbackslash\% reconstruction of training data, and good generalization ability. We validate our model by four tasks: molecular graph generation and reconstruction, visualization of the continuous latent space, property optimization, and constrained property optimization. Our MoFlow achieves state-of-the-art performance, which implies its potential efficiency and effectiveness to explore large chemical space for drug discovery.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Physics - Chemical Physics,Statistics - Machine Learning},
  file = {/home/phanthh/Academia/RP-Research Papers/storage/QSATYFT9/Zang and Wang - 2020 - MoFlow An Invertible Flow Model for Generating Mo.pdf}
}

@article{zhangDeepLearningGraphs2020,
  title = {Deep {{Learning}} on {{Graphs}}: {{A Survey}}},
  shorttitle = {Deep {{Learning}} on {{Graphs}}},
  author = {Zhang, Ziwei and Cui, Peng and Zhu, Wenwu},
  year = {2020},
  month = mar,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {34},
  number = {1},
  pages = {249--270},
  issn = {1558-2191},
  doi = {10.1109/TKDE.2020.2981333},
  abstract = {Deep learning has been shown to be successful in a number of domains, ranging from acoustics, images, to natural language processing. However, applying deep learning to the ubiquitous graph data is non-trivial because of the unique characteristics of graphs. Recently, substantial research efforts have been devoted to applying deep learning methods to graphs, resulting in beneficial advances in graph analysis techniques. In this survey, we comprehensively review the different types of deep learning methods on graphs. We divide the existing methods into five categories based on their model architectures and training strategies: graph recurrent neural networks, graph convolutional networks, graph autoencoders, graph reinforcement learning, and graph adversarial methods. We then provide a comprehensive overview of these methods in a systematic manner mainly by following their development history. We also analyze the differences and compositions of different methods. Finally, we briefly outline the applications in which they have been used and discuss potential future research directions.},
  keywords = {deep learning,Deep learning,Distance measurement,graph autoencoder,graph convolutional network,Graph data,graph neural network,Recurrent neural networks,Reinforcement learning,Social networking (online),Task analysis,Training},
  file = {/home/phanthh/Academia/RP-Research Papers/storage/446FNAH4/Zhang et al_2022_Deep Learning on Graphs.pdf;/home/phanthh/Academia/RP-Research Papers/storage/ZDKTD7LT/Zhang et al. - 2022 - Deep Learning on Graphs A Survey.pdf}
}

@article{zhouGraphNeuralNetworks2020,
  title = {Graph Neural Networks: {{A}} Review of Methods and Applications},
  shorttitle = {Graph Neural Networks},
  author = {Zhou, Jie and Cui, Ganqu and Hu, Shengding and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng and Sun, Maosong},
  year = {2020},
  month = jan,
  journal = {AI Open},
  volume = {1},
  pages = {57--81},
  issn = {2666-6510},
  doi = {10.1016/j.aiopen.2021.01.001},
  url = {https://www.sciencedirect.com/science/article/pii/S2666651021000012},
  urldate = {2022-06-17},
  abstract = {Lots of learning tasks require dealing with graph data which contains rich relation information among elements. Modeling physics systems, learning molecular fingerprints, predicting protein interface, and classifying diseases demand a model to learn from graph inputs. In other domains such as learning from non-structural data like texts and images, reasoning on extracted structures (like the dependency trees of sentences and the scene graphs of images) is an important research topic which also needs graph reasoning models. Graph neural networks (GNNs) are neural models that capture the dependence of graphs via message passing between the nodes of graphs. In recent years, variants of GNNs such as graph convolutional network (GCN), graph attention network (GAT), graph recurrent network (GRN) have demonstrated ground-breaking performances on many deep learning tasks. In this survey, we propose a general design pipeline for GNN models and discuss the variants of each component, systematically categorize the applications, and propose four open problems for future research.},
  langid = {english},
  keywords = {Deep learning,Graph neural network},
  file = {/home/phanthh/Academia/RP-Research Papers/storage/EPVVPVHX/Zhou et al_2020_Graph neural networks.pdf}
}


